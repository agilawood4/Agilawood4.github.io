<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>梯度下降算法 | MyBlog</title><meta name="keywords" content="Python与机器学习"><meta name="author" content="Destiny"><meta name="copyright" content="Destiny"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="梯度下降算法"><meta name="application-name" content="梯度下降算法"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="梯度下降算法"><meta property="og:url" content="http://example.com/2025/09/14/(NN)%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/index.html"><meta property="og:site_name" content="MyBlog"><meta property="og:description" content="梯度下降算法通过阅读本文，您可以大致了解最优化理论，梯度下降算法的原理与实现，常见的损失函数等相关内容 最优化与深度学习最优化目标我们所有的神经网络在训练过程中，都是要解决同一个优化问题，即寻找到神经网络上的一组参数 $\theta$，能够显著降低损失函数$$J(\theta)&amp;#x3D;E_{(x"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="http://example.com/images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A277.jpg"><meta property="article:author" content="Destiny"><meta property="article:tag"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://example.com/images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A277.jpg"><meta name="description" content="梯度下降算法通过阅读本文，您可以大致了解最优化理论，梯度下降算法的原理与实现，常见的损失函数等相关内容 最优化与深度学习最优化目标我们所有的神经网络在训练过程中，都是要解决同一个优化问题，即寻找到神经网络上的一组参数 $\theta$，能够显著降低损失函数$$J(\theta)&amp;#x3D;E_{(x"><link rel="shortcut icon" href="/favicon.ico"><link rel="canonical" href="http://example.com/2025/09/14/(NN)%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"><link rel="preconnect" href="//cdn.cbd.int"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: undefined,
  peoplecanvas: {"enable":true,"img":"/images/文章封面48.jpg"},
  postHeadAiDescription: {"enable":true,"gptName":"AnZhiYu","mode":"local","switchBtn":false,"btnLink":"https://afdian.net/item/886a79d4db6711eda42a52540025c377","randomNum":3,"basicWordCount":1000,"key":"xxxx","Referer":"https://xx.xx/"},
  diytitle: {"enable":true,"leaveTitle":"w(ﾟДﾟ)w 不要走！再看看嘛！","backTitle":"♪(^∇^*)欢迎肥来！"},
  LA51: undefined,
  greetingBox: undefined,
  twikooEnvId: '',
  commentBarrageConfig:undefined,
  music_page_default: "nav_music",
  root: '/',
  preloader: {"source":3},
  friends_vue_info: undefined,
  navMusic: true,
  mainTone: undefined,
  authorStatus: undefined,
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"作者: Destiny","link":"链接: ","source":"来源: MyBlog","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  shortcutKey: undefined,
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: 'MyBlog',
  title: '梯度下降算法',
  postAI: '',
  pageFillDescription: '梯度下降算法, 最优化与深度学习, 最优化目标, 训练误差和泛化误差, 经验风险, 优化中的挑战, 概念辨析, 损失函数, 最大似然估计, 交叉熵, 最大后验估计, 贝叶斯估计, 损失函数的性质, 可微与可导, 凸性, Jesen不等式, 梯度下降法概述, 搜索逼近策略, 梯度的概念, 随机梯度下降法, 算法描述, 算法评价, 动态学习率, 小批量梯度下降法, 算法描述, 算法评价, 决定批量大小的因素, 动量法, 算法概念, 算法评价, AdaGrad算法, 算法概念, 算法流程, 稀疏特征, 算法评价, RMSPropx2FAdaDelta算法, RMSProp算法, AdaDelta算法, Adam算法, 梯度下降算法演进流程, Adam算法, 梯度下降算法流程, 算法选择策略, 学习率调节器, 学习率衰减, 指数衰减, 余弦学习率调节, 预热, 参考代码梯度下降算法通过阅读本文您可以大致了解最优化理论梯度下降算法的原理与实现常见的损失函数等相关内容最优化与深度学习最优化目标我们所有的神经网络在训练过程中都是要解决同一个优化问题即寻找到神经网络上的一组参数能够显著降低损失函数损失函数通常包含训练集上的性能评价加上一个额外的正则化项训练误差和泛化误差训练误差指模型在训练数据上的误差泛化误差指模型在新数据上的误差深度学习模型关注的不仅是降低训练误差同时也要确保降低泛化误差而一般的最优化问题只需要关注降低训练误差即可经验风险经验风险把机器学习问题转化为一个优化问题的最简单方法就是最小化训练集上的期望损失这是深度学习中最基本的一个假设在统计学理论中我们要学习一个模型希望它在整个数据分布下表现最好理想情况下我们需要最小化期望风险也就是但是真实的分布是位置的我们没法直接这么计算我们受伤只有一个训练集我们考虑通过训练数据的平均损失来近似期望风险也就是这就是经验风险本质上它是训练集上的平均损失在实际训练中我们所做的就是经验风险的最小化优化中的挑战在深度学习的最优化问题中我们会遇到很多的挑战病态问题的解关于条件过于敏感数据中及其微妙的噪声也会使得结果发生剧烈变化系统的抗干扰能力很差局部最小值在某个局部区域内目标函数的值小于周围所有点的值梯度下降就可能在这些局部位置处停止或减小从而找不到全局真正的最小值鞍点模型训练过程中损失函数在某一点取得最小值或最大值但不是全局最优解悬崖梯度更新会很大程度上改变参数值使得参数弹射的非常远长期依赖梯度消失和爆炸深度结构使得模型丧失了学习到先前信息的能力让优化变得困难概念辨析在这里我们需要区分深度学习中两个重要的概念优化器和损失函数优化器损失函数最大似然估计最大后验估计最小均方误差交叉熵贝叶斯估计损失函数损失函数衡量预测值和真实值之间差异的函数每个模型都有学习目标而驱动模型朝着目标进行学习则是通过设计特定的损失函数使得模型最小化损失函数而得到的损失函数的起源可以追溯到统计学与最小二乘回归下面我们所提到的损失函数并不是这些具体的损失函数而是他们内部所实现的参数更新的机制和原理最大似然估计最大似然估计给定观测数据假设他们来自某个分布族其中是待估参数最大似然的想法就是把已经看到的数据当做既定事实在所有可能的中找到使得分布最接近数据真实分布的那个值通俗点讲我们就是要先猜一个和分布比较接近的分布族比如分布泊松分布等通过调整这些分布的参数使得我们猜的这个分布尽可能接近数据原本真实的分布求解步骤假设我们猜数据满足的分布是上的独立同分布我们写出似然函数为自变量因为数据满足独立同分布因此我们可以把联合概率拆分成若干个概率的乘积此时因为概率乘积的不稳定性比如如果有一个概率值很小那么总体乘积的结果则会收到较大的影响我们进行对数似然处理便于求导数值稳定而我们要做的最大似然估计就是找到使得值最大的参数注意似然是数据对参数的函数不是参数的概率假设这里我们选择的分布是高斯分布那么接下来就需要把高斯分布的表达式代入则有最大似然估计和对数似然估计如下我们观察最后一步的式子含有项的只有最后的均方误差项也就是说我们要求解的就是最小化均方误差由此可见当假设分布为高斯分布时最大似然估计可以转化为最小均方误差即后者是前者的一个特殊情况交叉熵从概率分布的角度来讲交叉熵损失也可以看成是最大似然估计对于我们上面化简得到的式子如果我们还不知道当前假设的分布族只是得到下面的形式化最大对数似然估计的表达式我们可以考虑对上式进行缩放变形也就是对每一个项我们都乘以一个概率表示当前的出现在所有样本中的频率也就是则我们的表达式就变成由于伸缩变换并不会影响对于最优值位置的确定因此这个变形和是等价的而变形之后的式子正是交叉熵损失函数的表达式最大后验估计最大后验估计从贝叶斯定理出发通过公式其中为后验分布在观测到数据以后我们对参数的认识为似然函数数据在参数下出现的可能性为先验分布在看到数据之前我们假定服从的分布为边际似然归一化常数和系数无关最大化后验概率要做的就是通过观测到的和之间的关系推断他们所满足的某个分布对应的参数也就是最大化代入贝叶斯公式其中分母为常数因此的目标可以化简为也就是说极大似然估计只考虑数据的似然函数即能够找到最大化观测数据的参数而最大后验估计在的基础上加入了先验分布也就是考虑了贝叶斯公式在数据似然的基础上同时考虑先验知识我们假设对取负对数转化为对数形式由此最大化概率即可转化为最小化负对数运算这里第一项对应的即为最大似然估计如果观测噪声是高斯分布则它就对应最小均方误差而第二项对应的就是正则化项且可以证明如果参数服从拉普拉斯分布即为正则如果服从高斯分布即为正则高贵的是如是总结的贝叶斯估计传统的数据分析分为两个学派频率派和贝叶斯派频率派认为数据是随机的但是参数是固定的常数采用极大似然估计求解参数贝叶斯派认为数据是客观存在已知的不是随机的数据通过观测值确定但参数本身是随机的服从某个分布使用贝叶斯分布求解参数贝叶斯估计用于估计参数的核心公式为其中左式后验分布表示在观测到数据以后我们对参数的新的认知右式分子似然先验其中似然表示数据在参数下出现的概率先验表示没有数据之前我们假定参数服从的分布分母证据作为归一化因子保证后验分布的归一化在参数优化时可以忽略因为它和本身无关贝叶斯估计的核心思想在于后验似然先验其合理的结合了先验经验与数据证据并且给出了参数不确定性的完整描述损失函数的性质我们在实际训练中使用的损失函数都具有若干共性下面我们会对常见损失函数的共有性质进行简单介绍可微与可导可微性函数在任意一点处都有一个导数可导性函数有连续的导函数在深度学习中我们有时候卡这两个概念没有那么严格比如常用的激活函数它在值处即不可导但是实际应用中位置出现的概率极低遇到时我们可以任选一个子梯度代替凸性凸函数保证函数有全局最小值可以使用较简单的优化算法找到这个最小值而凹函数则需要更加复杂的优化算法来寻找最小值损失函数要求必须是凸函数一个函数是否为凸函数可以通过凸性定理进行判定如果函数的二阶导数大于则为凸函数如果小于则为凹函数如果等于则为平函数我们常见的对数函数和最小均方误差函数都是凸函数凸优化使用凸优化算法来最小化凸函数的优化问题凸约束在优化问题中约束条件是凸函数现实中大多数待优化函数都不是凸函数我们需要借助凸优化找到其中为凸的部分再按照凸优化进行求解不等式不等式期望的函数小于等于凸函数的期望间其中凸函数的定义就是不等式的两点形式两点形式的不等式如图所示梯度下降法概述在损失函数中如果出现了函数对数指数函数等此时的损失函数被称为超越方程这类方程没有解析解只能通过优化分析算法中的搜索逼近的方式进行最优化求解而梯度下降就是其中最常用的方法梯度下降算法并不是独属于深度学习的算法而是最优化搜索算法适用于许多问题场景中假设我们有这样一个待求解的超越方程其中是我们待求解的使得表达式最小的参数而整个表达式记为损失函数我们先以为一维参数的情况进行简单演示也就是说我们要找到使得最小的一组的参数值搜索逼近策略在寻找最小损失函数值对应的参数时我们需要先确定方向即我们的参数列表中的每个的变化方向正方向还是负方向后确定步长即每个变化的幅度方向对应梯度的概念我们需要沿着梯度反方向进行参数调整步长对应学习率的概念通过设置学习率我们的步长不会过大或过小梯度的概念梯度描述的是一个函数曲面的陡度一般情况下我们的损失函数都是比较复杂的多元函数而偏导数则对应其在某一个具体方向上的陡度梯度即为函数在所有方向上偏导数的向量和比如对于该函数关于和的梯度分别为而函数的梯度我们一般用表示其中表示梯度算子梯度求解时也会用到链式求导法则假设那么则有由此梯度下降法的核心公式即为这里假设我们的训练集为损失函数为均方误差以均方误差为例则模型的在本轮训练中的全局损失为也就是对全部样本的损失函数值取平均得到的结果而梯度的计算是基于全局损失求解的结果因此本轮训练对于参数的梯度为也就是对每个样本在此时取梯度的平均值而在最终梯度更新参数的公式中其中为学习率为梯度为更新后的参数值为上一轮训练的参数值随机梯度下降法朴素的梯度下降法需要在整个数据集上进行求解往往存在以下问题不能保证优化函数达到全局最优解在全部数据集上训练最小化损失计算时间太长如果函数形态复杂则可能在局部最小值附近来回震荡对于初始值的选择比较敏感因此在实际训练中我们使用的往往是一些梯度下降法的变种算法以期规避这些问题算法描述随机梯度下降法在每次迭代中仅使用一个样本从整个数据集中随机抽取一个样本来计算梯度根据此梯度来调整参数的值即其中即为我们在每轮更新中选择用来计算梯度的样本算法评价优点计算速度快每次更新只取一个样本点计算量小相对不易陷入局部最优单个样本点的影响力度有限不易把参数直接拉到局部极值中缺点受噪声影响大某一轮梯度计算中可能选取到噪声点有方差大无法收敛情况动态学习率使用动态学习率可以帮助模型更快地收敛一般来讲我们设计的思路为在训练之初希望学习率大一些帮助模型快速收敛并且回避局部最优随着训练的进行学习率应逐渐减小防止模型震荡或错过最优解常见的动态学习率模型如下小批量梯度下降法算法描述小批量梯度下降法可以看做是对梯度下降每次取全体数据集和随机梯度下降每次只抽取一个样本的折中也就是每次在全体数据中随机抽取个样本来计算梯度这里我们同时更新权重和偏置两组系数都可以使用进行更新值得注意的是由于小批量梯度下降算法在实际应用中较多一般我们提到表示的就是小批量梯度下降而不再是朴素的梯度下降算法评价优点训练速度快震荡较少控制更加灵活缺点需要调整的参数变多学习率敏感批量大小通常在之间决定批量大小的因素批大小设置为是大量实验得到的结论通常考虑以下因素过大的批量虽然使得梯度估计更加精确但汇报少样本中可能有很多噪声或冗余信息增大批量和精度提高之间的性价比不高过小的批量难以充分利用多核架构性能接近随机梯度下降并行处理下内存消耗和批大小成正比批大小往往受到内存的制约的幂次方的批大小在使用时可以提高效率需要注意的是不论屁大小设置为多少我们每次都必须随机抽取在具体实验中我们每次抽取前往往会对数据进行操作使得数据被打乱动量法算法概念在深度学习中动量反映的是各个时刻模型梯度的累计和动量法的核心思想即为奖当前的梯度与上一步的梯度加权平均来减少梯度的震荡一阶动量过去各个时刻梯度的线性组合其中权重根据经验值一般设置为也就是更加偏向历史梯度累计的结果二阶动量过去各个时刻梯度的平方的线性组合算法评价我们提到的动量法一般指的都是基于一阶动量的梯度下降法也就是优点加速收敛可以帮助模型跳过局部最小值减少学习率过大导致的震荡风险减轻模型对初始权重的敏感度缺点需要维护动量项模型收敛速度变慢一个可以可视化动量对梯度影响的网站算法算法概念算法可以认为是使用二阶动量的梯度下降的改进算法其中二阶动量的出现也是自适应学习率优化算法时代到来的标志对于传统的梯度下降法所有参数都共享同一个固定的学习率在不同维度上收敛效率不同可能导致系数特征的梯度更新太慢高频特征的梯度震荡太大算法引入了自适应学习率其核心思想是为每个参数分配一个单独的学习率根并且据二阶动量进行动态调整算法流程首先计算历史梯度的平方和其中表示时刻的梯度接着计算每个参数对应的更新步长其中即为每个参数的一阶动量为初始设置的学习率这里有时为了防止分母除以会在分母项上加一个小常数作为噪声最后每个参数的梯度下降更新如下对于算法的直观理解自动缩放学习率梯度频繁较大的方向学习率会逐渐减小梯度稀疏的小方向学习率较大稀疏特征稀疏特征指的是在很多样本中只有少数出现过的特征在训练模型时稀疏特征可能很少更新导致训练不出理想的结果算法的核心思想也符合直观感知对于那些频繁被学习更新的参数我们后面就尽可能少更新它而对于那些不怎么被学习更新的参数我们后面就尽可能多更新它算法评价优点有效处理稀疏特征如自然语言处理推荐系统里的特征向量能够自动调整学习率缺点训练后期收敛速度变慢学习率调整方式固定下图中左侧为右侧为算法算法算法算法是对算法的改进其改变了二阶动量计算方法的策略不再累积全部历史梯度而是只关注过去一段时间窗口的下降梯度也就是在计算梯度平方和的时候在当前梯度平方和过去累计的梯度平方和之间加入了权重系数一般称为衰减率的权衡考虑模型评价自动调整学习率模型收敛更快实现上较为简单需要调整超参数如衰减率和学习率算法的更新机制为其中是历史梯度的平方累加和这会导致一个问题因为分母有累加效应会越来越大学习率会单调衰减到接近为对于稀疏特征比较好但是面对深度神经网络模型到后期可能还没达到最优就学不动了因此指出不要无限制累加而是使用指数衰减平均滑动窗口来代替累加和也就是维护了一个梯度平方的指数平均其中作为遗忘因子它会像一样只保留一个有限历史的平均值接着计算参数更新量其中表示当前历史累加梯度大小的估计表示当前步长更新大小的估计接着维护更新量的平方平均这样下一步我们就能使用来进行更新我们可以大致认为也就是使用指数衰减平均代替累加像并且用历史最新量的来动态缩放学习率避免了学习率逐渐消失的问题和都是对二阶动量进行优化的方法其基本思想都是时序累加修改二阶动量并且动态进行学习率的调整算法梯度下降算法演进流程算法是目前梯度下降中最常用的算法本身性能也很优越是先前若干梯度下降算法及其变种的集大成者其中在的基础上增加了一阶动量而在的基础上增加了二阶动量则同时使用了一阶动量和二阶动量即自适应动量估计算法是目前最主流也是最好用的梯度下降算法算法同时使用一阶动量梯度的加权平均和二阶动量梯度平方的加权平均来更新参数一阶动量估计参考动量法其中表示梯度的指数加权平均一般取历史经验二阶动量估计参考其中表示梯度平方的加权平均一般取历史经验偏差修正由于和在训练初期会偏向因此需要进行修正设置更新步长和方向其中表示更新的步长表示更新的方向是稳定项一般设置为学习率一般设置为历史经验参数更新能够加速收敛减少震荡同时实现自适应学习率在大多数深度学习任务上表现稳健是常用的默认优化器梯度下降算法流程我们对上述所有梯度下降算法进行总结可以发现其调整参数的处理流程有许多共性可以由以下的框架进行总结概括定义优化参数目标函数初始学习率开始每个迭代优化计算目标函数当前的梯度根据历史梯度计算一阶动量和二阶动量其中不同的算法变种的核心区别就在于动量的计算方式上的差异计算当前的方向和步长并且把二者相乘作为当前参数更新量迭代更新权重参数值得注意的是不同梯度下降算法的核心区别在于算法选择策略根据不同的训练场景我们会选择不同的梯度下降算法变种也就是选择不同的优化器不想做精细调优直接用希望更加自如地控制优化迭代的各类参数选用先使用快速下降到收敛范围内再使用精细调整到最优解算法核心思想优点缺点适用场景小批量梯度下降每次用一个小批量样本的梯度更新参数实现简单泛化能力好噪声帮助跳出局部最优收敛慢容易震荡需要手动调学习率小数据集凸优化问题作为为每个参数分配自适应学习率历史梯度平方累积调整步长稀疏特征收敛快减少手动调参学习率单调衰减后期几乎停滞推荐系统特征稀疏用指数滑动平均代替的累积使学习率不会消失能适应非平稳目标避免学习率过早衰减需要调参数理论收敛性弱非凸问题在线学习改进版引入历史更新量的自适应调整步长去掉全局学习率不需要手动设学习率收敛较稳健对超参数仍敏感复杂度比高深度学习早期应用学习率不易调的场景结合一阶动量二阶动量加偏差修正收敛快鲁棒性强几乎默认最优解有时泛化性能略差理论收敛性争议深度学习主流选择等学习率调节器学习率是各类优化算法中最关键的参数之一它会直接影响到网络训练的速度以及最终的收敛情况学习率调节器用于在训练过程中动态调整学习率这里需要辨析优化器和损失函数之间的关系它们是训练过程中两个完全不同但相互配合的模块损失函数衡量模型预测值和真实值之间的指标我们的训练的目标就是让损失函数尽可能小定义优化的目标优化器根据损失函数的梯度决定参数该如何更新是使得模型接近最优解的手段实现目标的方法在一个训练中前向传播得到预测值损失函数计算当前误差反向传播计算梯度优化器用梯度更新参数常见的学习率调节器学习率衰减指数衰减余弦学习率调节预热学习率衰减学习率衰减是指每训练一定次数比如个就把学习率降低一定的比例对应阶梯式的学习率下降曲线代码实现学习率衰减例如每训练次就将学习率降低为原来的一半指数衰减指数衰减指的是每次迭代时都把学习率乘上一个衰减率从而使学习率逐渐降低其中是初始学习率是衰减率为了防止梯度消失衰减率尽量不要太小为当前迭代的次数代码实现指数衰减法每次迭代将学习率乘上一个衰减率余弦学习率调节余弦学习率调节根据余弦函数来调节学习率使得学习率在训练过程中可以实现降低升高的循环往复而不是单调递减代码实现余弦学习率调节初始学习率为最大学习率是最小学习率是最大迭代次数预热预热指的是学习率从较小值逐渐提升到较大值的过程代码实现预热参考代码本章完整参考代码见网盘文件梯度下降算法',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-10-19 23:38:55',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"></head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><img class="loading-img nolazyload" alt="加载头像" src="https://npm.elemecdn.com/anzhiyu-blog-static@1.0.4/img/avatar.jpg"/><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.getElementById('loading-box').classList.add("loaded");
  },
  initLoading: () => {
    document.getElementById('loading-box').classList.remove("loaded")
  }
}
window.addEventListener('load',()=> { preloader.endLoading() })
setTimeout(function(){preloader.endLoading();},10000)

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"/><script async="async" src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><a id="site-name" href="/" accesskey="h"><div class="title">MyBlog</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)" rel="external nofollow noreferrer">PAGE_NAME</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/categories/"><span> 分类</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/tags/"><span> 标签</span></a></div></div></div><div id="nav-right"><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" alt="微信" src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" alt="支付宝" src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> 最新评论</span></div><div class="aside-list"><span>正在加载中...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/LLM%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">LLM与强化学习<sup>1</sup></a><a href="/tags/Python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">Python与机器学习<sup>35</sup></a><a href="/tags/editing/" style="font-size: 1.05rem;">editing<sup>1</sup></a><a href="/tags/%E4%B8%93%E4%B8%9A%E8%AF%BE/" style="font-size: 1.05rem;">专业课<sup>14</sup></a><a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 1.05rem;">算法<sup>29</sup></a></div></div><hr/></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/10/"><span class="card-archive-list-date">十月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">10</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/09/"><span class="card-archive-list-date">九月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">21</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/08/"><span class="card-archive-list-date">八月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">30</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/07/"><span class="card-archive-list-date">七月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">24</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);" rel="external nofollow noreferrer"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="post-meta-categories"><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-inbox post-meta-icon"></i><a class="post-meta-categories" href="/categories/Python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url">Python与机器学习</a></span><span class="article-meta tags"><a class="article-meta__tags" href="/tags/Python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Python与机器学习</span></a></span></div></div><h1 class="post-title" itemprop="name headline">梯度下降算法</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2025-09-14T07:26:06.000Z" title="发表于 2025-09-14 15:26:06">2025-09-14</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2025-10-19T15:38:55.305Z" title="更新于 2025-10-19 23:38:55">2025-10-19</time></span></div><div class="meta-secondline"><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为香波地群岛"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>香波地群岛</span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A277.jpg"></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container" itemscope itemtype="http://example.com/2025/09/14/(NN)%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"><header><a class="post-meta-categories" href="/categories/Python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url">Python与机器学习</a><a href="/tags/Python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" tabindex="-1" itemprop="url">Python与机器学习</a><h1 id="CrawlerTitle" itemprop="name headline">梯度下降算法</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">Destiny</span><time itemprop="dateCreated datePublished" datetime="2025-09-14T07:26:06.000Z" title="发表于 2025-09-14 15:26:06">2025-09-14</time><time itemprop="dateCreated datePublished" datetime="2025-10-19T15:38:55.305Z" title="更新于 2025-10-19 23:38:55">2025-10-19</time></header><h1 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a><strong>梯度下降算法</strong></h1><p>通过阅读本文，您可以大致了解最优化理论，梯度下降算法的原理与实现，常见的损失函数等相关内容</p>
<h2 id="最优化与深度学习"><a href="#最优化与深度学习" class="headerlink" title="最优化与深度学习"></a><strong>最优化与深度学习</strong></h2><h3 id="最优化目标"><a href="#最优化目标" class="headerlink" title="最优化目标"></a><strong>最优化目标</strong></h3><p>我们所有的神经网络在训练过程中，都是要解决同一个<strong>优化问题</strong>，即寻找到神经网络上的一组参数 $\theta$，能够显著降低损失函数<br>$$<br>J(\theta)&#x3D;E_{(x,y)\sim\hat{p_{data}}}L(f(x;\theta),y)<br>$$<br>损失函数通常包含训练集上的性能评价，加上一个额外的正则化项</p>
<h3 id="训练误差和泛化误差"><a href="#训练误差和泛化误差" class="headerlink" title="训练误差和泛化误差"></a><strong>训练误差和泛化误差</strong></h3><p>训练误差（Training Error）：指模型在训练数据上的误差</p>
<p>泛化误差（Generalization Error）：指模型在新数据上的误差<br>$$<br>E_{train}&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^N(y_i-f(x_i))^2\<br>E_{gen}&#x3D;E_{x,y}[(y-f(x))^2]<br>$$<br>深度学习模型关注的不仅是降低训练误差，同时也要确保降低泛化误差，而一般的最优化问题只需要关注降低训练误差即可</p>
<h3 id="经验风险"><a href="#经验风险" class="headerlink" title="经验风险"></a><strong>经验风险</strong></h3><p>经验风险：把机器学习问题转化为一个优化问题的最简单方法就是<strong>最小化</strong>训练集上的期望损失，这是深度学习中最基本的一个假设</p>
<p>在统计学理论中，我们要学习一个模型 $f_\theta(x)$，希望它在整个数据分布 $P(x,y)$ 下表现最好，理想情况下，我们需要最小化<strong>期望风险</strong>（Expected Risk），也就是：<br>$$<br>R(\theta)&#x3D;E_{(x,y)\sim P}[L(f_\theta(x),y)]<br>$$<br>但是真实的分布 $P(x,y)$ 是位置的，我们没法直接这么计算，我们受伤只有一个训练集，$D&#x3D;{(x^{(i)},y^{(i)})}<em>{i&#x3D;1}^N$，我们考虑通过训练数据的平均损失来<strong>近似</strong>期望风险，也就是：<br>$$<br>R</em>{emp}(\theta)&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^NL(f_\theta(x^{(i)},y^{(i)})<br>$$<br>这就是<strong>经验风险</strong>，本质上，它是训练集上的平均损失，在实际训练中，我们所做的就是经验风险的最小化（ERM）</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E7%BB%8F%E9%AA%8C%E9%A3%8E%E9%99%A91.png" alt="image-20250915123740339"></p>
<h3 id="优化中的挑战"><a href="#优化中的挑战" class="headerlink" title="优化中的挑战"></a><strong>优化中的挑战</strong></h3><p>在深度学习的最优化问题中，我们会遇到很多的挑战：</p>
<ul>
<li>病态（ill-conditioned problem）：问题的解关于条件过于敏感，数据中及其微妙的噪声也会使得结果发生剧烈变化，系统的抗干扰能力很差</li>
<li>局部最小值（Local Minima）：在某个局部区域内，目标函数的值小于周围所有点的值，梯度下降就可能在这些局部位置处停止或减小，从而找不到全局真正的最小值</li>
<li>鞍点（Saddle point）：模型训练过程中，损失函数在某一点取得最小值或最大值，但不是全局最优解</li>
<li>悬崖（clipping）：梯度更新会很大程度上改变参数值，使得参数弹射的非常远</li>
<li>长期依赖（梯度消失和爆炸）：深度结构使得模型丧失了学习到先前信息的能力，让优化变得困难</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E6%88%96%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B81.png" alt="image-20250915151730039"></p>
<h3 id="概念辨析"><a href="#概念辨析" class="headerlink" title="概念辨析"></a><strong>概念辨析</strong></h3><p>在这里，我们需要区分深度学习中两个重要的概念：优化器和损失函数：</p>
<ul>
<li>优化器：SGD，AdaGrad，AdaDelt，RMSProp，Adam，NAdam</li>
<li>损失函数：最大似然估计，最大后验估计，最小均方误差，交叉熵，贝叶斯估计</li>
</ul>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a><strong>损失函数</strong></h2><p>损失函数（loss function）：衡量预测值和真实值之间差异的函数，每个模型都有学习目标，而驱动模型朝着目标进行学习则是通过设计特定的损失函数，使得模型最小化损失函数而得到的</p>
<p>损失函数的起源可以追溯到统计学与最小二乘回归：<br>$$<br>{\arg\max}<em>\theta\sum</em>{i&#x3D;1}^n(y_i-f(x_i;\theta))^2<br>$$</p>
<p>下面我们所提到的损失函数，并不是 Sigmoid，Adam，SGD 这些具体的损失函数，而是他们内部所实现的参数更新的机制和原理</p>
<h3 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a><strong>最大似然估计</strong></h3><p>最大似然估计（Maxinum Likelihood Estimation，MLE）：给定观测数据 $x_1,x_2,\dots,x_n$，<strong>假设</strong>他们来自某个分布族 $f(x|\theta)$，其中 θ 是待估参数，<strong>最大似然</strong>的想法就是把已经看到的数据当做<strong>既定事实</strong>，在所有可能的 θ 中，找到使得 $f(x|\theta)$ 分布最接近数据真实分布的那个 θ 值</p>
<p>通俗点讲，我们就是要先<strong>猜一个</strong>和 $x_1,x_2,\dots,x_n$ 分布比较接近的分布族（比如 Gauss 分布，泊松分布等），通过调整这些分布的参数，使得我们猜的这个分布尽可能接近数据原本真实的分布</p>
<p>求解步骤：假设我们猜数据满足的分布是 $f(x_i|\theta)$ 上的独立同分布，我们写出<strong>似然函数</strong>（θ 为自变量）：<br>$$<br>\displaystyle L(\theta;x)&#x3D;f(x_1, x_2, \dots,x_n|\theta)&#x3D;\prod_{i&#x3D;1}^nf(x_i|\theta)<br>$$<br>因为数据满足独立同分布，因此我们可以把联合概率拆分成若干个概率的<strong>乘积</strong>，此时因为概率乘积的不稳定性（比如如果有一个概率值很小，那么总体乘积的结果则会收到较大的影响），我们进行对数似然处理（便于求导，数值稳定）：<br>$$<br>l(\theta)&#x3D;\log L(\theta)&#x3D;\sum_{i&#x3D;1}^n\log f(x_i|\theta)<br>$$<br>而我们要做的<strong>最大似然估计</strong>（MLE）就是找到使得 $l(\theta)$ 值最大的参数 θ（注意：似然是数据对参数的函数，不是参数的概率）：<br>$$<br>\hat{\theta}<em>{MLE}&#x3D;\arg\max</em>{\theta}l(\theta)<br>$$<br>假设这里我们选择的分布是高斯分布，那么接下来就需要把高斯分布的表达式代入，则有最大似然估计和对数似然估计如下：<br>$$<br>L(\theta,\sigma^2)<br>&#x3D; \prod_{i&#x3D;1}^n \frac{1}{\sqrt{2\pi\sigma^2}}<br>\exp!\left(-\frac{(y_i-\mathbf{x}_i^{!\top}\theta)^2}{2\sigma^2}\right)<br>&#x3D; (2\pi\sigma^2)^{-\tfrac{n}{2}}<br>\exp!\left(-\frac{1}{2\sigma^2},|y-X\theta|_2^2\right)<br>$$</p>
<p>$$<br>\ell(\theta,\sigma^2)&#x3D;\log L<br>&#x3D; -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log\sigma^2</p>
<ul>
<li>\frac{1}{2\sigma^2}\underbrace{|y - X\theta|<em>2^2}</em>{\text{MSE}(\theta)}<br>$$</li>
</ul>
<p>我们观察最后一步的式子，含有 θ 项的只有最后的均方误差项， 也就是说，我们要求解的就是<strong>最小化均方误差</strong>（MSE），由此可见，当假设分布为高斯分布时，最大似然估计可以转化为最小均方误差，即后者是前者的一个特殊情况</p>
<h3 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a><strong>交叉熵</strong></h3><p>从概率分布的角度来讲，交叉熵损失也可以看成是最大似然估计 MLE</p>
<p>对于我们上面化简得到的式子，如果我们还不知道当前假设的分布族 $f(x_i|\theta)$，只是得到下面的形式化最大对数似然估计的表达式：<br>$$<br>l(\theta)&#x3D;\log L(\theta)&#x3D;\sum_{i&#x3D;1}^n\log f(x_i|\theta)<br>$$<br>我们可以考虑对上式进行<strong>缩放变形</strong>，也就是对每一个 $f(x_i|\theta)$ 项，我们都乘以一个概率 $\hat{p_{x_i}}$，表示当前的 $x_i$ 出现在所有样本中的频率，也就是 $|x_i| &#x2F; |x_1, x_2, \dots,x_n|$，则我们的表达式就变成：<br>$$<br>l(\theta)&#x3D;\sum_{i&#x3D;1}^n\hat{p_{data}(x)}\log f(x_i|\theta)<br>$$<br>由于伸缩变换并不会影响对于最优值 θ 位置的确定，因此这个变形和 MLE 是等价的，而变形之后的式子，正是<strong>交叉熵</strong>损失函数的表达式</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E4%BA%A4%E5%8F%89%E7%86%B5%E5%92%8CMLE.png" alt="image-20250914162831102"></p>
<h3 id="最大后验估计"><a href="#最大后验估计" class="headerlink" title="最大后验估计"></a><strong>最大后验估计</strong></h3><p>最大后验估计（Maximum A Posteriori Estimation，MAP）从贝叶斯定理出发，通过公式：<br>$$<br>P(\theta|X,Y)&#x3D;\frac{P(Y,X|\theta)f(\theta)}{P(Y,X)}<br>$$<br>其中：</p>
<ul>
<li>$P(\theta|X,Y)$ 为后验分布（在观测到数据 X Y 以后，我们对参数的认识）</li>
<li>$P(Y,X|\theta)$ 为似然函数（数据在参数 θ 下出现的可能性）</li>
<li>$f(\theta)$ 为先验分布（在看到数据之前，我们假定 θ 服从的分布）</li>
<li>$P(Y,X)$ 为边际似然（归一化常数，和 θ 系数无关）</li>
</ul>
<p>最大化后验概率要做的就是，通过观测到的 X 和 Y 之间的关系，推断他们所满足的某个分布对应的参数 θ，也就是最大化 $P(\theta|X,Y)$，代入贝叶斯公式，其中分母为常数，因此 MAP 的目标可以化简为：<br>$$<br>\hat{\theta}<em>{MAP}&#x3D;\arg\max</em>{\theta}P(Y,X|\theta)f(\theta)<br>$$<br>也就是说，极大似然估计（MLE）只考虑数据的似然函数，即能够找到最大化观测数据的参数 θ，而最大后验估计（MAP）在 MLE 的基础上加入了<strong>先验分布</strong> $f(\theta)$，也就是考虑了贝叶斯公式：在数据似然的基础上，同时考虑先验知识</p>
<p>我们假设 $\theta$ ~ $f(\theta)$，对 MAP 取<strong>负对数</strong>，转化为对数形式，由此最大化概率即可转化为最小化负对数运算：<br>$$<br>\arg\max_{\theta}[-\log P(Y,X|\theta)-\log f(\theta)]<br>$$<br>这里第一项对应的即为 MLE（最大似然估计），如果观测噪声 $\log P(Y,X|\theta)$ 是高斯分布，则它就对应 MSE（最小均方误差），而第二项 $\log f(\theta)$ 对应的就是<strong>正则化</strong>项，且可以证明如果参数 θ 服从拉普拉斯分布，即为 L1 正则，如果服从高斯分布，即为 L2 正则</p>
<p>高贵的 chat-gpt 是如是总结的：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/MLE%E5%92%8CMAP%E5%AF%B9%E6%AF%94.png" alt="image-20250914220709114"></p>
<h3 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a><strong>贝叶斯估计</strong></h3><p>传统的数据分析分为两个学派，频率派（Frequentist）和贝叶斯派（Bayesian）：</p>
<ul>
<li>频率派：认为数据是随机的，但是参数 θ 是<strong>固定</strong>的常数，采用极大似然估计（MLE）求解参数 θ</li>
<li>贝叶斯派：认为数据是客观存在，已知的，不是随机的（数据通过观测值确定），但参数本身是<strong>随机</strong>的，服从某个分布，使用贝叶斯分布求解参数 θ</li>
</ul>
<p>贝叶斯估计（Bayesian Estimation）用于估计参数的核心公式为：<br>$$<br>P(\theta|x^{(1)},x^{(2)},\dots,x^{(m)})&#x3D;\frac{P(x^{(1)},x^{(2)},\dots,x^{(m)})f(\theta)}{P(x^{(1)},x^{(2)},\dots,x^{(m)})}<br>$$<br>其中：</p>
<ul>
<li>左式：后验分布，表示在观测到数据 x 以后，我们对参数 θ 的新的认知</li>
<li>右式分子：似然（Likelihood） × 先验（Prior），其中似然表示数据在参数 θ 下出现的概率，先验表示没有数据之前，我们假定参数服从的分布</li>
<li>分母：证据（Evidence），作为归一化因子，保证后验分布的归一化，在参数优化时可以忽略，因为它和 θ 本身无关</li>
</ul>
<p>贝叶斯估计的核心思想在于：后验 &lt;- 似然×先验，其合理的结合了先验经验与数据证据，并且给出了参数不确定性的完整描述</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E4%B8%89%E7%A7%8D%E5%B8%B8%E7%94%A8%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png" alt="image-20250914222356482"></p>
<h2 id="损失函数的性质"><a href="#损失函数的性质" class="headerlink" title="损失函数的性质"></a><strong>损失函数的性质</strong></h2><p>我们在实际训练中使用的损失函数都具有若干共性，下面我们会对常见损失函数的共有性质进行简单介绍</p>
<h3 id="可微与可导"><a href="#可微与可导" class="headerlink" title="可微与可导"></a><strong>可微与可导</strong></h3><p>可微性（differentiability）：函数在任意一点处都有一个导数</p>
<p>可导性（continuity）：函数有<strong>连续</strong>的导函数</p>
<p>在深度学习中我们有时候卡这两个概念没有那么严格，比如常用的 ReLU 激活函数，它在 0 值处即不可导，但是实际应用中 0 位置出现的概率极低，遇到时我们可以任选一个子梯度代替</p>
<h3 id="凸性"><a href="#凸性" class="headerlink" title="凸性"></a><strong>凸性</strong></h3><p>凸函数（Convex）保证函数有全局最小值，可以使用较简单的优化算法找到这个最小值；而凹函数（Concave）则需要更加复杂的优化算法来寻找最小值</p>
<p>损失函数要求必须是<strong>凸函数</strong></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E5%87%B8%E5%87%BD%E6%95%B0.png" alt="image-20250914223049440"></p>
<p>一个函数是否为凸函数可以通过<strong>凸性定理</strong>进行判定：如果函数的二阶导数大于 0，则为凸函数；如果小于 0，则为凹函数，如果等于 0，则为平函数</p>
<p>我们常见的对数函数和最小均方误差函数都是凸函数：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E5%B8%B8%E8%A7%81%E7%9A%84%E5%87%B8%E5%87%BD%E6%95%B0.png" alt="image-20250914223330137"></p>
<p>凸优化：使用凸优化算法来最小化凸函数的优化问题</p>
<p>凸约束：在优化问题中，约束条件是凸函数（现实中大多数待优化函数都不是凸函数，我们需要借助凸优化，找到其中为凸的部分，再按照凸优化进行求解）</p>
<h3 id="Jesen不等式"><a href="#Jesen不等式" class="headerlink" title="Jesen不等式"></a><strong>Jesen不等式</strong></h3><p>Jesen不等式：期望的函数小于等于凸函数的期望间，其中凸函数的定义就是 Jesen 不等式的两点形式<br>$$<br>tf(x_1)+(1-t)f(x_2)&gt;&#x3D;f(tx_1+(1-t)x_2)<br>$$<br>两点形式的 Jesen 不等式如图所示：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/Jesen%E4%B8%8D%E7%AD%89%E5%BC%8F.png" alt="image-20250914223921930"></p>
<h2 id="梯度下降法概述"><a href="#梯度下降法概述" class="headerlink" title="梯度下降法概述"></a><strong>梯度下降法概述</strong></h2><p>在损失函数中，如果出现了 sigmoid 函数，对数指数函数等，此时的损失函数被称为<strong>超越方程</strong>，这类方程没有解析解，只能通过优化分析算法中的<strong>搜索逼近</strong>的方式进行最优化求解，而梯度下降就是其中最常用的方法</p>
<p>梯度下降算法并不是独属于深度学习的算法，而是最优化搜索算法，适用于许多问题场景中</p>
<p>假设我们有这样一个待求解的超越方程：<br>$$<br>w*&#x3D;\arg\min_w{\frac{1}{m}L(y_i,f(x_i;w))}<br>$$</p>
<p>其中 w* 是我们待求解的使得表达式最小的参数，而整个表达式记为损失函数 J，我们先以 w 为一维参数的情况进行简单演示，也就是说我们要找到使得 J 最小的一组 w 的参数值：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.png" alt="image-20250915082031841"></p>
<h3 id="搜索逼近策略"><a href="#搜索逼近策略" class="headerlink" title="搜索逼近策略"></a><strong>搜索逼近策略</strong></h3><p>在寻找最小损失函数值对应的 w 参数时，我们需要先确定方向，即我们的参数列表中的每个 w 的变化方向（正方向还是负方向），后确定步长（即每个 w 变化的幅度）：</p>
<ul>
<li>方向：对应梯度的概念，我们需要沿着梯度反方向进行参数调整</li>
<li>步长：对应学习率的概念，通过设置学习率，我们的步长不会过大或过小</li>
</ul>
<h3 id="梯度的概念"><a href="#梯度的概念" class="headerlink" title="梯度的概念"></a><strong>梯度的概念</strong></h3><p>梯度（Gradient）描述的是一个函数曲面的陡度（一般情况下，我们的损失函数都是比较复杂的多元函数），而偏导数则对应其在某一个具体方向上的陡度</p>
<p>梯度即为函数在所有方向上偏导数的<strong>向量和</strong>，比如对于 $f(x)&#x3D;0.5x^2+y^2$，该函数关于 $x$ 和 $y$ 的梯度分别为：<br>$$<br>\frac{\delta f(x,y)}{\delta x}&#x3D;x,;\frac{\delta f(x,y)}{\delta y}&#x3D;2y<br>$$<br>而函数的梯度我们一般用 $\nabla f(x,y) &#x3D; \begin{bmatrix} x \ 2y \end{bmatrix}$ 表示，其中 $\nabla$ 表示梯度算子</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%A2%AF%E5%BA%A6%E7%9A%84%E5%9B%BE.png" alt="image-20250915082917671"></p>
<p>梯度求解时也会用到链式求导法则，假设：<br>$$<br>J(w_0,w_1,\dots,w_n)&#x3D;f[g_1(<em>),g_2(</em>),\dots,g_n(<em>)],;</em>&#x3D;(w_0,w_1,\dots,w_n)<br>$$<br>那么则有：<br>$$<br>\frac{\delta J}{\delta w_i}&#x3D;\sum_{j}^{M}\frac{\delta f}{\delta g_j}\frac{\delta g_j}{\delta w_i}<br>$$<br>由此，梯度下降法的<strong>核心</strong>公式即为：<br>$$<br>w_{t+1}&#x3D;w_t-\alpha\nabla J<br>$$<br>这里假设我们的训练集为：<br>$$<br>{x^{(i)},y^{(i)}}<em>{i&#x3D;1}^N<br>$$<br>损失函数 $L$ 为均方误差（以均方误差为例），则模型的在本轮训练中的全局损失 $J$ 为：<br>$$<br>J(\theta)&#x3D;\frac{1}{N}\sum</em>{i&#x3D;1}^N L(h_\theta(x^{(i)}),y^{(i)})<br>$$<br>也就是对全部样本的损失函数值取<strong>平均</strong>得到的结果，而梯度的计算是基于全局损失 $J$ 求解的结果，因此本轮训练对于参数 θ 的梯度为：<br>$$<br>\nabla_\theta J(\theta)&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^N\nabla_\theta L(h_\theta(x^{(i)}),y^{(i)})<br>$$<br>也就是对每个样本在此时取梯度的平均值，而在最终梯度更新参数的公式中，其中 $\alpha$ 为学习率，$\nabla J$ 为梯度，$w_{t+1}$ 为更新后的参数值，$w_t$ 为上一轮训练的参数值</p>
<h3 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a><strong>随机梯度下降法</strong></h3><p>朴素的梯度下降法需要在整个数据集上进行求解，往往存在以下问题：</p>
<ul>
<li>不能保证优化函数达到全局最优解</li>
<li>在全部数据集上训练最小化损失，计算时间太长</li>
<li>如果函数形态复杂，则可能在局部最小值附近来回震荡</li>
<li>对于初始值的选择比较敏感</li>
</ul>
<p>因此在实际训练中，我们使用的往往是一些梯度下降法的变种算法，以期规避这些问题</p>
<h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a><strong>算法描述</strong></h4><p>随机梯度下降法（Stochastic Gradient Descent，SGD）在每次迭代中仅使用<strong>一个样本</strong>（从整个数据集中随机抽取一个样本）来计算梯度，根据此梯度来调整参数的值，即：<br>$$<br>w_{t+1}&#x3D;w_t-\alpha\nabla L(w;x_i,y_i)<br>$$<br>其中 $(x_i,y_i)$ 即为我们在每轮更新中，选择用来计算梯度的样本</p>
<h4 id="算法评价"><a href="#算法评价" class="headerlink" title="算法评价"></a><strong>算法评价</strong></h4><p>SGD 优点：</p>
<ul>
<li>计算速度快（每次更新只取一个样本点，计算量小）</li>
<li>相对不易陷入局部最优（单个样本点的影响力度有限，不易把参数直接拉到局部极值中）</li>
</ul>
<p>缺点：</p>
<ul>
<li>受噪声影响大（某一轮梯度计算中可能选取到噪声点）</li>
<li>有方差大无法收敛情况</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%A2%AF%E5%BA%A6%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6.png" alt="image-20250915084533346"></p>
<h4 id="动态学习率"><a href="#动态学习率" class="headerlink" title="动态学习率"></a><strong>动态学习率</strong></h4><p>使用动态学习率可以帮助模型更快地收敛，一般来讲我们设计的思路为：在训练之初，希望学习率大一些，帮助模型快速收敛并且回避局部最优，随着训练的进行，学习率应逐渐减小，防止模型震荡或错过最优解</p>
<p>常见的动态学习率模型如下：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E5%8A%A8%E6%80%81%E5%AD%A6%E4%B9%A0%E7%8E%87.png" alt="image-20250915084836781"></p>
<h3 id="小批量梯度下降法"><a href="#小批量梯度下降法" class="headerlink" title="小批量梯度下降法"></a><strong>小批量梯度下降法</strong></h3><h4 id="算法描述-1"><a href="#算法描述-1" class="headerlink" title="算法描述"></a><strong>算法描述</strong></h4><p>小批量梯度下降法（Mini-Batch Stochastic Gradient Descent）可以看做是对梯度下降（每次取全体数据集）和随机梯度下降（每次只抽取一个样本）的<strong>折中</strong>，也就是每次在全体数据中随机抽取 m 个样本来计算梯度：<br>$$<br>w_{t+1}&#x3D;w_t-\alpha\frac{1}{m}\sum_{i&#x3D;1}^m\nabla L(w,b,x_i,y_i)\b_{t+1}&#x3D;b_t-\alpha\frac{1}{m}\sum_{i&#x3D;1}^m\nabla L(w,b,x_i,y_i)<br>$$<br>这里我们同时更新权重和偏置两组系数，都可以使用 SGD 进行更新，值得<strong>注意</strong>的是，由于小批量梯度下降算法在实际应用中较多，一般我们提到 SGD，表示的就是小批量梯度下降，而不再是朴素的梯度下降</p>
<h4 id="算法评价-1"><a href="#算法评价-1" class="headerlink" title="算法评价"></a><strong>算法评价</strong></h4><p>优点：</p>
<ul>
<li>训练速度快</li>
<li>震荡较少</li>
<li>控制更加灵活</li>
</ul>
<p>缺点：</p>
<ul>
<li>需要调整的参数变多</li>
<li>学习率敏感</li>
<li>批量大小通常在 32 - 256 之间</li>
</ul>
<h4 id="决定批量大小的因素"><a href="#决定批量大小的因素" class="headerlink" title="决定批量大小的因素"></a><strong>决定批量大小的因素</strong></h4><p>批大小设置为 32 - 256 是大量实验得到的结论，通常考虑以下因素：</p>
<ul>
<li>过大的批量虽然使得梯度估计更加精确，但汇报少，样本中可能有很多噪声或冗余信息，增大批量和精度提高之间的性价比不高</li>
<li>过小的批量难以充分利用多核架构，性能接近随机梯度下降</li>
<li>并行处理下，内存消耗和批大小成正比，批大小往往受到内存的制约</li>
<li>2的幂次方的批大小在使用 GPU 时可以提高效率</li>
</ul>
<p>需要注意的是，不论屁大小设置为多少，我们每次都必须<strong>随机抽取</strong>，在具体实验中，我们每次抽取前往往会对数据进行<code>shuffle()</code>操作，使得数据被打乱</p>
<h2 id="动量法"><a href="#动量法" class="headerlink" title="动量法"></a><strong>动量法</strong></h2><h3 id="算法概念"><a href="#算法概念" class="headerlink" title="算法概念"></a><strong>算法概念</strong></h3><p>在深度学习中，动量（Momentum）反映的是各个时刻模型梯度的累计和，动量法的核心思想即为奖当前的梯度与上一步的梯度<strong>加权平均</strong>来减少梯度的震荡</p>
<ul>
<li>一阶动量：过去各个时刻梯度的线性组合，其中 $\beta$ 权重根据经验值一般设置为 0.9，也就是更加偏向历史梯度累计的结果</li>
</ul>
<p>$$<br>m_t &#x3D; \beta_1m_{t-1}+(1-\beta_1)g_t<br>$$</p>
<ul>
<li>二阶动量：过去各个时刻梯度的平方的线性组合</li>
</ul>
<p>$$<br>V_t &#x3D; \sum_{T&#x3D;1}^t g_t^2<br>$$</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E5%8A%A8%E9%87%8F%E6%B3%95.png" alt="image-20250915092056325"></p>
<h3 id="算法评价-2"><a href="#算法评价-2" class="headerlink" title="算法评价"></a><strong>算法评价</strong></h3><p>我们提到的<strong>动量法</strong>，一般指的都是基于一阶动量的梯度下降法，也就是：$m_t&#x3D;\beta_1m_{t-1}+(1-\beta_1)g_t,;w&#x3D;w-\alpha m_t$</p>
<p>优点：</p>
<ul>
<li>加速收敛，可以帮助模型跳过局部最小值</li>
<li>减少学习率过大导致的震荡风险</li>
<li>减轻模型对初始权重的敏感度</li>
</ul>
<p>缺点：</p>
<ul>
<li>需要维护动量项，模型收敛速度变慢</li>
</ul>
<p>一个可以可视化动量对梯度影响的网站：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://distill.pub/2017/momentum/">Why Momentum Really Works (distill.pub)</a></p>
<h2 id="AdaGrad算法"><a href="#AdaGrad算法" class="headerlink" title="AdaGrad算法"></a><strong>AdaGrad算法</strong></h2><h3 id="算法概念-1"><a href="#算法概念-1" class="headerlink" title="算法概念"></a><strong>算法概念</strong></h3><p>AdaGrad 算法可以认为是使用二阶动量的梯度下降的改进算法，其中二阶动量的出现也是自适应学习率优化算法时代到来的标志</p>
<p>对于传统的梯度下降法，所有参数都共享同一个固定的学习率 $\alpha$，在不同维度上收敛效率不同，可能导致：</p>
<ul>
<li>系数特征的梯度更新太慢</li>
<li>高频特征的梯度震荡太大</li>
</ul>
<p>AdaGrad 算法引入了<strong>自适应学习率</strong>，其核心思想是为每个参数分配一个单独的学习率，根并且据二阶动量进行动态调整</p>
<h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><strong>算法流程</strong></h3><p>首先计算历史梯度的平方和：<br>$$<br>V_t &#x3D; \sum_{T&#x3D;1}^t g_T^2 ;;;or;;;V_t &#x3D; V_{t-1}+g_T<em>g_T<br>$$<br>其中 $g_T$ 表示 T 时刻的梯度，接着计算每个参数对应的更新步长：<br>$$<br>\eta&#x3D;\alpha</em>\frac{m_t}{\sqrt{V_t}}<br>$$<br>其中 $m_t$ 即为每个参数的一阶动量，$\alpha$ 为初始设置的学习率，这里有时为了防止分母除以 0，会在分母项上加一个 $\epsilon$ 小常数作为噪声，最后每个参数的梯度下降更新如下：<br>$$<br>w_{t+1} &#x3D; w_t - \eta_t<br>$$<br>对于 AdaGrad 算法的直观理解：自动缩放学习率，梯度频繁较大的方向，学习率会逐渐减小，梯度稀疏的小方向，学习率较大</p>
<h3 id="稀疏特征"><a href="#稀疏特征" class="headerlink" title="稀疏特征"></a><strong>稀疏特征</strong></h3><p>稀疏特征指的是在很多样本中只有少数出现过的特征，在训练模型时，稀疏特征可能很少更新，导致训练不出理想的结果</p>
<p>AdaGrad 算法的核心思想也符合直观感知：对于那些频繁被学习更新的参数，我们后面就尽可能少更新它；而对于那些不怎么被学习更新的参数，我们后面就尽可能多更新它</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E7%A8%80%E7%96%8F%E7%89%B9%E5%BE%81.png" alt="image-20250915094532419"></p>
<h3 id="算法评价-3"><a href="#算法评价-3" class="headerlink" title="算法评价"></a><strong>算法评价</strong></h3><p>优点：</p>
<ul>
<li>有效处理稀疏特征（如自然语言处理、推荐系统里的特征向量）</li>
<li>能够自动调整学习率</li>
</ul>
<p>缺点：</p>
<ul>
<li>训练后期收敛速度变慢</li>
<li>学习率调整方式固定</li>
</ul>
<p>下图中左侧为 SGD，右侧为 AdaGrad 算法：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E5%AF%B9%E6%AF%94SGD%E5%92%8CAdaGrad.png" alt="image-20250915094809341"></p>
<h2 id="RMSProp-AdaDelta算法"><a href="#RMSProp-AdaDelta算法" class="headerlink" title="RMSProp&#x2F;AdaDelta算法"></a><strong>RMSProp&#x2F;AdaDelta算法</strong></h2><h3 id="RMSProp算法"><a href="#RMSProp算法" class="headerlink" title="RMSProp算法"></a><strong>RMSProp算法</strong></h3><p>RMSProp 算法是对 AdaGrad 算法的改进，其改变了二阶动量计算方法的策略，不再累积全部历史梯度，而是只关注过去一段时间窗口的下降梯度<br>$$<br>V_t &#x3D; \beta_2<em>V_{t-1}+(1-\beta_2)g_t^2 \<br>\eta_t &#x3D; \alpha m_t &#x2F; \sqrt{V_t} \<br>w_{t+1} &#x3D; w_t - \eta_t<br>$$<br>也就是在计算梯度平方和的时候，在当前梯度平方和过去累计的梯度平方和之间加入了权重系数 β（一般称为*<em>衰减率</em></em>）的权衡考虑</p>
<p>模型评价：</p>
<ul>
<li>自动调整学习率，模型收敛更快</li>
<li>实现上较为简单</li>
<li>需要调整超参数，如衰减率（β）和学习率（α）</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/RMSProp.png" alt="image-20250915101244477"></p>
<h3 id="AdaDelta算法"><a href="#AdaDelta算法" class="headerlink" title="AdaDelta算法"></a><strong>AdaDelta算法</strong></h3><p>AdaGrad 的更新机制为：<br>$$<br>\theta_{t+1}&#x3D;\theta_t-\frac{\alpha*m_t}{\sqrt{r_t}+\epsilon}<br>$$<br>其中 $r_t$ 是历史梯度的平方累加和，这会导致一个问题：因为分母有累加效应，$r_t$ 会越来越大，学习率会单调衰减到接近为 0，对于稀疏特征比较好，但是面对深度神经网络，模型到后期可能还没达到最优就学不动了</p>
<p>因此 AdaDelta 指出，不要无限制累加，而是使用<strong>指数衰减平均</strong>（滑动窗口）来代替累加和，也就是维护了一个<strong>梯度平方</strong>的<strong>指数平均</strong>：<br>$$<br>E[g^2]<em>t&#x3D;\rho E[g^2]</em>{t-1}+(1-\rho)g_t^2<br>$$<br>其中 $\rho$ 作为遗忘因子，它会像 RMS 一样，只保留一个有限历史的平均值，接着计算参数更新量：<br>$$<br>\Delta\theta_t&#x3D;-\frac{RMS[\Delta\theta]_{t-1}}{RMS[g]<em>t}g_t \<br>\theta_t&#x3D;\theta</em>{t-1}+\Delta\theta<br>$$<br>其中：</p>
<ul>
<li>$RMS[g]_{t}&#x3D;\sqrt{E[g^2]_t+\epsilon}$ 表示当前历史累加梯度大小的估计</li>
<li>$RMS[\Delta\theta]<em>{t-1}&#x3D;\sqrt{E[\Delta\theta^2]</em>{t-1}+\epsilon}$ 表示当前步长更新大小的估计</li>
</ul>
<p>接着维护<strong>更新量</strong>的平方平均：<br>$$<br>E[\Delta\theta^2]<em>t&#x3D;\rho E[\Delta\theta^2]</em>{t-1}+(1-\rho)(\Delta\theta_t)^2<br>$$<br>这样，下一步我们就能使用 $RMS[\Delta\theta]_t$ 来进行更新，我们可以大致认为 AdaDelta &#x3D; AdaGra、d + RMS，也就是使用指数衰减平均代替累加像，并且用历史最新量的 RMS 来动态缩放学习率，避免了学习率逐渐消失的问题</p>
<p>RMSProp 和 AdaDelta 都是对二阶动量进行优化的方法，其基本思想都是时序累加修改二阶动量，并且动态进行学习率的调整</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E5%B0%8F%E7%BB%931.png" alt="image-20250915101520152"></p>
<h2 id="Adam算法"><a href="#Adam算法" class="headerlink" title="Adam算法"></a><strong>Adam算法</strong></h2><h3 id="梯度下降算法演进流程"><a href="#梯度下降算法演进流程" class="headerlink" title="梯度下降算法演进流程"></a><strong>梯度下降算法演进流程</strong></h3><p>Adam算法是目前梯度下降中最常用的算法，本身性能也很优越，是先前若干梯度下降算法及其变种的集大成者，其中 SGDM 在 SGD 的基础上增加了一阶动量，而 AdaGrad、AdaDelta、RMS 在 SGD 的基础上增加了二阶动量，Adam 则<strong>同时</strong>使用了一阶动量和二阶动量，即 Adaptive + Momentum（自适应+动量估计）</p>
<p>Adam 算法是目前最主流，也是最好用的梯度下降算法</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%BC%94%E8%BF%9B%E6%B5%81%E7%A8%8B.png" alt="image-20250915104106283"></p>
<h3 id="Adam算法-1"><a href="#Adam算法-1" class="headerlink" title="Adam算法"></a><strong>Adam算法</strong></h3><p>Adam 同时使用一阶动量（梯度的加权平均）和二阶动量（梯度平方的加权平均）来更新参数</p>
<p>（1）一阶动量估计（参考动量法）：<br>$$<br>m_t&#x3D;\beta_1m_{t-1}+(1-\beta_1)g_t<br>$$<br>其中 $m_t$ 表示梯度的指数加权平均，$\beta_1$ 一般取 0.9（）历史经验</p>
<p>（2）二阶动量估计（参考 RMSProp）：<br>$$<br>V_t&#x3D;\beta_2 V_{t-1}+(1-\beta_2)g_t^2<br>$$<br>其中 $V_t$ 表示梯度平方的加权平均，$\beta_2$ 一般取 0.999（历史经验）</p>
<p>（3）偏差修正：由于 $m_t$ 和 $V_t$ 在训练初期会偏向 0，因此需要进行修正<br>$$<br>\hat{m_t} &#x3D; \frac{m_t}{1-\beta_1^t}<br>\<br>\hat{V_t}&#x3D;\frac{V_t}{1-\beta_2^t}<br>$$<br>（4）设置更新步长和方向：<br>$$<br>\eta &#x3D; \frac{\alpha}{\sqrt{\hat{V_t}}+\epsilon}\hat{m_t}<br>$$<br>其中 $\frac{\alpha}{\sqrt{\hat{V_t}}+\epsilon}$ 表示更新的步长，$\hat{m_t}$ 表示更新的方向，$\epsilon$ 是稳定项，一般设置为 1e-8，学习率 $\alpha$ 一般设置为 0.001（历史经验）</p>
<p>（5）参数更新：<br>$$<br>\theta_{t+1}&#x3D;\theta_t - \eta<br>$$<br>Adam 能够加速收敛，减少震荡，同时实现自适应学习率，在大多数深度学习任务上表现稳健，是常用的默认优化器</p>
<h3 id="梯度下降算法流程"><a href="#梯度下降算法流程" class="headerlink" title="梯度下降算法流程"></a><strong>梯度下降算法流程</strong></h3><p>我们对上述所有梯度下降算法进行总结，可以发现其调整参数的处理流程有许多共性，可以由以下的框架进行总结概括：</p>
<p>定义优化参数 $w$，目标函数 $f(w)$，初始学习率 $\alpha$，开始每个 epoch 迭代优化</p>
<ul>
<li>计算目标函数当前的梯度 $g_t&#x3D;\nabla f(w_t)$</li>
<li>根据历史梯度计算一阶动量和二阶动量 $m_t&#x3D;\phi(g_1,g_2,\dots,g_t),;V_t&#x3D;\psi(g_1,g_2,\dots,g_t)$，其中不同的算法变种的核心区别就在于动量的计算方式上的差异</li>
<li>计算当前的方向 $m_t$ 和步长 $\alpha&#x2F;\sqrt{V_t}$，并且把二者相乘作为当前参数更新量 $\eta_t$</li>
<li>迭代更新权重参数 $w_{t+1}&#x3D;w_t-\eta_t$</li>
</ul>
<p>值得注意的是，不同梯度下降算法的核心区别在于 $n_t&#x3D;(\alpha&#x2F;\sqrt{V_t})m_t$</p>
<h3 id="算法选择策略"><a href="#算法选择策略" class="headerlink" title="算法选择策略"></a><strong>算法选择策略</strong></h3><p>根据不同的训练场景，我们会选择不同的梯度下降算法变种，也就是选择不同的<strong>优化器</strong>：</p>
<ul>
<li>不想做精细调优，直接用 Adam</li>
<li>希望更加自如地控制优化迭代的各类参数，选用 SGD</li>
<li>先使用 Adam 快速下降到收敛范围内，再使用 SGD 精细调整到最优解</li>
</ul>
<table>
<thead>
<tr>
<th>算法</th>
<th>核心思想</th>
<th>优点</th>
<th>缺点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>SGD</strong> (小批量梯度下降)</td>
<td>每次用一个小批量样本的梯度更新参数</td>
<td>实现简单；泛化能力好；噪声帮助跳出局部最优</td>
<td>收敛慢；容易震荡；需要手动调学习率</td>
<td>小数据集；凸优化问题；作为 baseline</td>
</tr>
<tr>
<td><strong>AdaGrad</strong></td>
<td>为每个参数分配自适应学习率，历史梯度平方累积调整步长</td>
<td>稀疏特征收敛快；减少手动调参</td>
<td>学习率单调衰减，后期几乎停滞</td>
<td>NLP、推荐系统（特征稀疏）</td>
</tr>
<tr>
<td><strong>RMSProp</strong></td>
<td>用指数滑动平均代替 AdaGrad 的累积，使学习率不会消失</td>
<td>能适应非平稳目标；避免学习率过早衰减</td>
<td>需要调 β\beta 参数；理论收敛性弱</td>
<td>RNN、非凸问题、在线学习</td>
</tr>
<tr>
<td><strong>AdaDelta</strong></td>
<td>RMSProp 改进版，引入历史更新量的 RMS，自适应调整步长，去掉全局学习率</td>
<td>不需要手动设学习率；收敛较稳健</td>
<td>对超参数仍敏感；复杂度比 RMSProp 高</td>
<td>深度学习早期应用，学习率不易调的场景</td>
</tr>
<tr>
<td><strong>Adam</strong></td>
<td>结合 Momentum (一阶动量) + RMSProp (二阶动量)，加偏差修正</td>
<td>收敛快；鲁棒性强；几乎默认最优解</td>
<td>有时泛化性能略差；理论收敛性争议</td>
<td><strong>深度学习主流选择</strong>，CV、NLP、GAN 等</td>
</tr>
</tbody></table>
<h2 id="学习率调节器"><a href="#学习率调节器" class="headerlink" title="学习率调节器"></a><strong>学习率调节器</strong></h2><p>学习率是各类优化算法中最关键的参数之一，它会直接影响到网络训练的速度，以及最终的收敛情况，学习率调节器用于在训练过程中<strong>动态调整</strong>学习率</p>
<p>这里需要辨析优化器（Optimizer）和损失函数（Loss Function）之间的关系，它们是训练过程中两个完全不同但<strong>相互配合</strong>的模块</p>
<ul>
<li>损失函数：衡量模型预测值和真实值之间的<strong>指标</strong>，我们的训练的目标就是让损失函数尽可能小（定义优化的目标）</li>
<li>优化器：根据损失函数的梯度，决定参数该如何更新，是使得模型接近最优解的<strong>手段</strong>（实现目标的方法）</li>
<li>在一个 epoch 训练中：前向传播得到预测值，损失函数计算当前误差，反向传播计算梯度，优化器用梯度更新参数</li>
</ul>
<p>常见的学习率调节器：学习率衰减、指数衰减、余弦学习率调节、预热</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E8%8A%82%E5%99%A8.png" alt="image-20250915111939943"></p>
<h3 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a><strong>学习率衰减</strong></h3><p>学习率衰减（Learning Decay）是指每训练一定次数（比如 100 个 Epoch）就把学习率降低一定的比例，对应阶梯式的学习率下降曲线</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F.png" alt="image-20250915112108763"></p>
<p>代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 学习率衰减，例如每训练100次就将学习率降低为原来的一半</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=<span class="number">100</span>, gamma=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>

<h3 id="指数衰减"><a href="#指数衰减" class="headerlink" title="指数衰减"></a><strong>指数衰减</strong></h3><p>指数衰减（Exp Decay）指的是每次迭代时都把学习率乘上一个<strong>衰减率</strong>，从而使学习率逐渐降低：<br>$$<br>lr &#x3D; lr_0*decay_rate^{global_step}<br>$$<br>其中 $lr_0$ 是初始学习率，$decay_{rate}$ 是衰减率（为了防止梯度消失，衰减率尽量不要太小），$global_{step}$ 为当前迭代的次数</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%8C%87%E6%95%B0%E5%AD%A6%E4%B9%A0%E7%8E%871.png" alt="image-20250915121304794"></p>
<p>代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指数衰减法，每次迭代将学习率乘上一个衰减率</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=<span class="number">0.99</span>)</span><br></pre></td></tr></table></figure>

<h3 id="余弦学习率调节"><a href="#余弦学习率调节" class="headerlink" title="余弦学习率调节"></a><strong>余弦学习率调节</strong></h3><p>余弦学习率调节（Cosine Annealing）根据余弦函数来调节学习率，使得学习率在训练过程中可以实现降低-升高的循环往复，而不是单调递减<br>$$<br>lr_t&#x3D;lr_{min}+\frac{lr_{max}-lr_{min}}{2}(1+\cos(\frac{T_{cur}}{T_{max}}\pi))<br>$$<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E4%BD%99%E5%BC%A6%E5%AD%A6%E4%B9%A0%E7%8E%87.png" alt="image-20250915121242621"></p>
<p>代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 余弦学习率调节，optimizer初始学习率为最大学习率，eta_min是最小学习率，T_max是最大迭代次数</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=<span class="number">100</span>, eta_min=<span class="number">0.00001</span>)</span><br></pre></td></tr></table></figure>

<h3 id="预热"><a href="#预热" class="headerlink" title="预热"></a><strong>预热</strong></h3><p>预热指的是学习率从较小值逐渐提升到较大值的过程<br>$$<br>lr_t&#x3D;lr_{min}+\frac{lr_{max}+lr_{min}}{steps}t<br>$$<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E9%A2%84%E7%83%AD%E5%AD%A6%E4%B9%A0%E7%8E%87.png" alt="image-20250915121401740"></p>
<p>代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预热</span></span><br><span class="line">warmup_steps = <span class="number">20</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=<span class="keyword">lambda</span> t: <span class="built_in">min</span>(t / warmup_steps, <span class="number">0.001</span>))</span><br></pre></td></tr></table></figure>

<h2 id="参考代码"><a href="#参考代码" class="headerlink" title="参考代码"></a><strong>参考代码</strong></h2><p>本章完整参考代码见网盘文件<strong>梯度下降算法</strong></p>
<hr>
</article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/images/picture.jpg" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/images/picture.jpg" title="头像" alt="头像"></a><div class="post-copyright__author_name">Destiny</div><div class="post-copyright__author_desc">跋涉浮尘烛光灭</div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="http://example.com/2025/09/14/(NN)%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('http://example.com/2025/09/14/(NN)%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/')">梯度下降算法</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"><div class="post-reward" onclick="anzhiyu.addRewardMask()"><div class="reward-button button--animated" title="赞赏作者"><i class="anzhiyufont anzhiyu-icon-hand-heart-fill"></i>打赏作者</div><div class="reward-main"><div class="reward-all"><span class="reward-title">感谢你赐予我前进的力量</span><ul class="reward-group"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul><a class="reward-main-btn" href="/about/#about-reward" target="_blank"><div class="reward-text">赞赏者名单</div><div class="reward-dec">因为你们的支持让我意识到写文章的价值🙏</div></a></div></div></div><div id="quit-box" onclick="anzhiyu.removeRewardMask()" style="display: none"></div></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="http://example.com/2025/09/14/(NN)%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=梯度下降算法&amp;url=http://example.com/2025/09/14/(NN)%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/&amp;pic=/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A277.jpg" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><script>function copyCurrentPageUrl() {
  var currentPageUrl = window.location.href;
  var input = document.createElement("input");
  input.setAttribute("value", currentPageUrl);
  document.body.appendChild(input);
  input.select();
  input.setSelectionRange(0, 99999);
  document.execCommand("copy");
  document.body.removeChild(input);
}</script><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="copyCurrentPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">MyBlog</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"><a class="post-meta__box__tags" href="/tags/Python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>Python与机器学习<span class="tagsPageCount">35</span></a></div></div></div><div class="post_share"><div class="social-share" data-image="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A2105.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/09/12/NLP/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A274.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">NLP</div></div></a></div><div class="next-post pull-right"><a href="/2025/09/14/(NN)%E7%BD%91%E7%BB%9C%E6%A6%82%E5%BF%B5%E8%A1%A5%E5%85%85/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A278.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">神经网络补充概念</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="anzhiyufont anzhiyu-icon-thumbs-up fa-fw" style="font-size: 1.5rem; margin-right: 4px"></i><span>喜欢这篇文章的人也看了</span></div><div class="relatedPosts-list"><div><a href="/2025/09/24/(ML)%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/" title="聚类算法"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A292.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-09-24</div><div class="title">聚类算法</div></div></a></div><div><a href="/2025/09/15/(CNN)%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" title="卷积神经网络基础"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A284.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-09-15</div><div class="title">卷积神经网络基础</div></div></a></div><div><a href="/2025/09/14/(NN)%E7%BD%91%E7%BB%9C%E6%A6%82%E5%BF%B5%E8%A1%A5%E5%85%85/" title="神经网络补充概念"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A278.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-09-14</div><div class="title">神经网络补充概念</div></div></a></div><div><a href="/2025/08/11/(NN)%E7%BD%91%E7%BB%9C%E6%A6%82%E5%BF%B5%E5%9F%BA%E7%A1%80/" title="神经网络基础概念"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A247.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-08-11</div><div class="title">神经网络基础概念</div></div></a></div><div><a href="/2025/08/20/(RNN)%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="基础循环神经网络"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A256.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-08-20</div><div class="title">基础循环神经网络</div></div></a></div><div><a href="/2025/08/04/(TF)Transformer/" title="Transformer"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A235.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-08-04</div><div class="title">Transformer</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/images/picture.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__description"></div></div></div><div class="card-widget anzhiyu-right-widget" id="card-wechat" onclick="null"><div id="flip-wrapper"><div id="flip-content"><div class="face" style="background: url(https://bu.dusays.com/2023/01/13/63c02edf44033.png) center center / 100% no-repeat"></div><div class="back face" style="background: url(https://bu.dusays.com/2023/05/13/645fa415e8694.png) center center / 100% no-repeat"></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>文章目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="toc-number">1.</span> <span class="toc-text">梯度下降算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%80%E4%BC%98%E5%8C%96%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.1.</span> <span class="toc-text">最优化与深度学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87"><span class="toc-number">1.1.1.</span> <span class="toc-text">最优化目标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E5%92%8C%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE"><span class="toc-number">1.1.2.</span> <span class="toc-text">训练误差和泛化误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%8F%E9%AA%8C%E9%A3%8E%E9%99%A9"><span class="toc-number">1.1.3.</span> <span class="toc-text">经验风险</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E4%B8%AD%E7%9A%84%E6%8C%91%E6%88%98"><span class="toc-number">1.1.4.</span> <span class="toc-text">优化中的挑战</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E5%BF%B5%E8%BE%A8%E6%9E%90"><span class="toc-number">1.1.5.</span> <span class="toc-text">概念辨析</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.</span> <span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1"><span class="toc-number">1.2.1.</span> <span class="toc-text">最大似然估计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5"><span class="toc-number">1.2.2.</span> <span class="toc-text">交叉熵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1"><span class="toc-number">1.2.3.</span> <span class="toc-text">最大后验估计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1"><span class="toc-number">1.2.4.</span> <span class="toc-text">贝叶斯估计</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E6%80%A7%E8%B4%A8"><span class="toc-number">1.3.</span> <span class="toc-text">损失函数的性质</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AF%E5%BE%AE%E4%B8%8E%E5%8F%AF%E5%AF%BC"><span class="toc-number">1.3.1.</span> <span class="toc-text">可微与可导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%B8%E6%80%A7"><span class="toc-number">1.3.2.</span> <span class="toc-text">凸性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Jesen%E4%B8%8D%E7%AD%89%E5%BC%8F"><span class="toc-number">1.3.3.</span> <span class="toc-text">Jesen不等式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%A6%82%E8%BF%B0"><span class="toc-number">1.4.</span> <span class="toc-text">梯度下降法概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%90%9C%E7%B4%A2%E9%80%BC%E8%BF%91%E7%AD%96%E7%95%A5"><span class="toc-number">1.4.1.</span> <span class="toc-text">搜索逼近策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-number">1.4.2.</span> <span class="toc-text">梯度的概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">1.4.3.</span> <span class="toc-text">随机梯度下降法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%8F%8F%E8%BF%B0"><span class="toc-number">1.4.3.1.</span> <span class="toc-text">算法描述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E8%AF%84%E4%BB%B7"><span class="toc-number">1.4.3.2.</span> <span class="toc-text">算法评价</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8A%A8%E6%80%81%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">1.4.3.3.</span> <span class="toc-text">动态学习率</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">1.4.4.</span> <span class="toc-text">小批量梯度下降法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%8F%8F%E8%BF%B0-1"><span class="toc-number">1.4.4.1.</span> <span class="toc-text">算法描述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E8%AF%84%E4%BB%B7-1"><span class="toc-number">1.4.4.2.</span> <span class="toc-text">算法评价</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%B3%E5%AE%9A%E6%89%B9%E9%87%8F%E5%A4%A7%E5%B0%8F%E7%9A%84%E5%9B%A0%E7%B4%A0"><span class="toc-number">1.4.4.3.</span> <span class="toc-text">决定批量大小的因素</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A8%E9%87%8F%E6%B3%95"><span class="toc-number">1.5.</span> <span class="toc-text">动量法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%A6%82%E5%BF%B5"><span class="toc-number">1.5.1.</span> <span class="toc-text">算法概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E8%AF%84%E4%BB%B7-2"><span class="toc-number">1.5.2.</span> <span class="toc-text">算法评价</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AdaGrad%E7%AE%97%E6%B3%95"><span class="toc-number">1.6.</span> <span class="toc-text">AdaGrad算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%A6%82%E5%BF%B5-1"><span class="toc-number">1.6.1.</span> <span class="toc-text">算法概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B"><span class="toc-number">1.6.2.</span> <span class="toc-text">算法流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A8%80%E7%96%8F%E7%89%B9%E5%BE%81"><span class="toc-number">1.6.3.</span> <span class="toc-text">稀疏特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E8%AF%84%E4%BB%B7-3"><span class="toc-number">1.6.4.</span> <span class="toc-text">算法评价</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RMSProp-AdaDelta%E7%AE%97%E6%B3%95"><span class="toc-number">1.7.</span> <span class="toc-text">RMSProp&#x2F;AdaDelta算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RMSProp%E7%AE%97%E6%B3%95"><span class="toc-number">1.7.1.</span> <span class="toc-text">RMSProp算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AdaDelta%E7%AE%97%E6%B3%95"><span class="toc-number">1.7.2.</span> <span class="toc-text">AdaDelta算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adam%E7%AE%97%E6%B3%95"><span class="toc-number">1.8.</span> <span class="toc-text">Adam算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E6%BC%94%E8%BF%9B%E6%B5%81%E7%A8%8B"><span class="toc-number">1.8.1.</span> <span class="toc-text">梯度下降算法演进流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adam%E7%AE%97%E6%B3%95-1"><span class="toc-number">1.8.2.</span> <span class="toc-text">Adam算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B"><span class="toc-number">1.8.3.</span> <span class="toc-text">梯度下降算法流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E9%80%89%E6%8B%A9%E7%AD%96%E7%95%A5"><span class="toc-number">1.8.4.</span> <span class="toc-text">算法选择策略</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E8%8A%82%E5%99%A8"><span class="toc-number">1.9.</span> <span class="toc-text">学习率调节器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F"><span class="toc-number">1.9.1.</span> <span class="toc-text">学习率衰减</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E6%95%B0%E8%A1%B0%E5%87%8F"><span class="toc-number">1.9.2.</span> <span class="toc-text">指数衰减</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%99%E5%BC%A6%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E8%8A%82"><span class="toc-number">1.9.3.</span> <span class="toc-text">余弦学习率调节</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E7%83%AD"><span class="toc-number">1.9.4.</span> <span class="toc-text">预热</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E4%BB%A3%E7%A0%81"><span class="toc-number">1.10.</span> <span class="toc-text">参考代码</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/10/25/RLHF/" title="(一)RLHF_PPO"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A2105.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="(一)RLHF_PPO"/></a><div class="content"><a class="title" href="/2025/10/25/RLHF/" title="(一)RLHF_PPO">(一)RLHF_PPO</a><time datetime="2025-10-25T02:45:26.000Z" title="发表于 2025-10-25 10:45:26">2025-10-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/23/%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/" title="语义分析和代码生成">语义分析和代码生成</a><time datetime="2025-10-23T07:29:17.000Z" title="发表于 2025-10-23 15:29:17">2025-10-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/21/%EF%BC%88%E7%BC%96%E8%AF%91%EF%BC%89%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/" title="语义分析和代码生成"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A2103.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="语义分析和代码生成"/></a><div class="content"><a class="title" href="/2025/10/21/%EF%BC%88%E7%BC%96%E8%AF%91%EF%BC%89%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/" title="语义分析和代码生成">语义分析和代码生成</a><time datetime="2025-10-21T08:31:25.000Z" title="发表于 2025-10-21 16:31:25">2025-10-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/21/(SQL)%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/" title="数据库设计"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A2102.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据库设计"/></a><div class="content"><a class="title" href="/2025/10/21/(SQL)%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/" title="数据库设计">数据库设计</a><time datetime="2025-10-21T01:50:34.000Z" title="发表于 2025-10-21 09:50:34">2025-10-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/16/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90Day1/" title="算法设计与分析Day1"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A2100.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="算法设计与分析Day1"/></a><div class="content"><a class="title" href="/2025/10/16/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90Day1/" title="算法设计与分析Day1">算法设计与分析Day1</a><time datetime="2025-10-16T14:11:44.000Z" title="发表于 2025-10-16 22:11:44">2025-10-16</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2020 - 2025 By <a class="footer-bar-link" href="/" title="Destiny" target="_blank">Destiny</a></div></div><div id="footer-type-tips"></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu" title="主题">主题</a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">文章</div><div class="length-num">85</div></a><a href="/tags/" title="tag"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/" title="category"><div class="headline">分类</div><div class="length-num">4</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" rel="external nofollow noreferrer" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.anheyu.com/" title="博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/favicon.ico" alt="博客"/><span class="back-menu-item-text">博客</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener external nofollow noreferrer" href="https://image.anheyu.com/" title="安知鱼图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="安知鱼图床"/><span class="back-menu-item-text">安知鱼图床</span></a></div></div></div><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/categories/"><span> 分类</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/tags/"><span> 标签</span></a></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/LLM%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">LLM与强化学习<sup>1</sup></a><a href="/tags/Python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">Python与机器学习<sup>35</sup></a><a href="/tags/editing/" style="font-size: 0.88rem;">editing<sup>1</sup></a><a href="/tags/%E4%B8%93%E4%B8%9A%E8%AF%BE/" style="font-size: 0.88rem;">专业课<sup>14</sup></a><a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 0.88rem;">算法<sup>29</sup></a></div></div><hr/></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">播放音乐</a><div id="console-music-bg"></div><meting-js id="8152976493" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random" volume="0.7"></meting-js></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8802438608&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script>// 消除控制台打印
var HoldLog = console.log;
console.log = function () {};
let now1 = new Date();
queueMicrotask(() => {
  const Log = function () {
    HoldLog.apply(console, arguments);
  }; //在恢复前输出日志
  const grt = new Date("04/01/2021 00:00:00"); //此处修改你的建站时间或者网站上线时间
  now1.setTime(now1.getTime() + 250);
  const days = (now1 - grt) / 1000 / 60 / 60 / 24;
  const dnum = Math.floor(days);
  const ascll = [
    `欢迎使用安知鱼!`,
    `生活明朗, 万物可爱`,
    `
        
       █████╗ ███╗   ██╗███████╗██╗  ██╗██╗██╗   ██╗██╗   ██╗
      ██╔══██╗████╗  ██║╚══███╔╝██║  ██║██║╚██╗ ██╔╝██║   ██║
      ███████║██╔██╗ ██║  ███╔╝ ███████║██║ ╚████╔╝ ██║   ██║
      ██╔══██║██║╚██╗██║ ███╔╝  ██╔══██║██║  ╚██╔╝  ██║   ██║
      ██║  ██║██║ ╚████║███████╗██║  ██║██║   ██║   ╚██████╔╝
      ╚═╝  ╚═╝╚═╝  ╚═══╝╚══════╝╚═╝  ╚═╝╚═╝   ╚═╝    ╚═════╝
        
        `,
    "已上线",
    dnum,
    "天",
    "©2020 By 安知鱼 V1.6.14",
  ];
  const ascll2 = [`NCC2-036`, `调用前置摄像头拍照成功，识别为【小笨蛋】.`, `Photo captured: `, `🤪`];

  setTimeout(
    Log.bind(
      console,
      `\n%c${ascll[0]} %c ${ascll[1]} %c ${ascll[2]} %c${ascll[3]}%c ${ascll[4]}%c ${ascll[5]}\n\n%c ${ascll[6]}\n`,
      "color:#425AEF",
      "",
      "color:#425AEF",
      "color:#425AEF",
      "",
      "color:#425AEF",
      ""
    )
  );
  setTimeout(
    Log.bind(
      console,
      `%c ${ascll2[0]} %c ${ascll2[1]} %c \n${ascll2[2]} %c\n${ascll2[3]}\n`,
      "color:white; background-color:#4fd953",
      "",
      "",
      'background:url("https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/tinggge.gif") no-repeat;font-size:450%'
    )
  );

  setTimeout(Log.bind(console, "%c WELCOME %c 你好，小笨蛋.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(
      console,
      "%c ⚡ Powered by 安知鱼 %c 你正在访问 Destiny 的博客.",
      "color:white; background-color:#f0ad4e",
      ""
    )
  );

  setTimeout(Log.bind(console, "%c W23-12 %c 你已打开控制台.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(console, "%c S013-782 %c 你现在正处于监控中.", "color:white; background-color:#d9534f", "")
  );
});</script><script async src="/anzhiyu/random.js"></script><div class="js-pjax"><input type="hidden" name="page-type" id="page-type" value="post"></div><script async data-pjax src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.1/bubble/bubble.js"></script><script>var visitorMail = "";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/dist/fireworks.min.js"></script><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>