<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>行人重识别 | MyBlog</title><meta name="keywords" content="Python与机器学习"><meta name="author" content="Destiny"><meta name="copyright" content="Destiny"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="行人重识别"><meta name="application-name" content="行人重识别"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="行人重识别"><meta property="og:url" content="http://example.com/2025/08/04/%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%E5%AD%A6%E4%B9%A0/index.html"><meta property="og:site_name" content="MyBlog"><meta property="og:description" content="行人重识别（基于Relation-Aware Global Attention论文学习与代码复现）一、任务介绍1.1 什么是行人重识别在计算机视觉领域中，行人重识别主要指的是在当前摄像头中识别到某个人，我们需要检测在其余哪些摄像头中这个人再次出现，其核心人物用到对图像特征的有效提取，以及对图像特征之"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="http://example.com/images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A233.jpg"><meta property="article:author" content="Destiny"><meta property="article:tag"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://example.com/images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A233.jpg"><meta name="description" content="行人重识别（基于Relation-Aware Global Attention论文学习与代码复现）一、任务介绍1.1 什么是行人重识别在计算机视觉领域中，行人重识别主要指的是在当前摄像头中识别到某个人，我们需要检测在其余哪些摄像头中这个人再次出现，其核心人物用到对图像特征的有效提取，以及对图像特征之"><link rel="shortcut icon" href="/favicon.ico"><link rel="canonical" href="http://example.com/2025/08/04/%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%E5%AD%A6%E4%B9%A0/"><link rel="preconnect" href="//cdn.cbd.int"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: undefined,
  peoplecanvas: {"enable":true,"img":"/images/文章封面48.jpg"},
  postHeadAiDescription: {"enable":true,"gptName":"AnZhiYu","mode":"local","switchBtn":false,"btnLink":"https://afdian.net/item/886a79d4db6711eda42a52540025c377","randomNum":3,"basicWordCount":1000,"key":"xxxx","Referer":"https://xx.xx/"},
  diytitle: {"enable":true,"leaveTitle":"w(ﾟДﾟ)w 不要走！再看看嘛！","backTitle":"♪(^∇^*)欢迎肥来！"},
  LA51: undefined,
  greetingBox: undefined,
  twikooEnvId: '',
  commentBarrageConfig:undefined,
  music_page_default: "nav_music",
  root: '/',
  preloader: {"source":3},
  friends_vue_info: undefined,
  navMusic: true,
  mainTone: undefined,
  authorStatus: undefined,
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"作者: Destiny","link":"链接: ","source":"来源: MyBlog","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  shortcutKey: undefined,
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: 'MyBlog',
  title: '行人重识别',
  postAI: '',
  pageFillDescription: '行人重识别（基于Relation-Aware Global Attention论文学习与代码复现）, 一、任务介绍, 1.1 什么是行人重识别, 1.2 研究背景, 1.3 常用数据集, 二、评估标准, 2.1 rank1标准, 2.2 map标准, 2.3 损失函数, 三、核心算法, 3.1 channel attention, 3.2 spatial attention, 3.3 核心特征提取流程, 四、代码复现, 4.1 数据集选取, 4.2 超参数设置, 4.3 核心代码框架, 五、模型评估与展望, 5.1 结果评估, 5.2 模型改进与后续展望, 六、参考文献行人重识别基于论文学习与代码复现一任务介绍什么是行人重识别在计算机视觉领域中行人重识别主要指的是在当前摄像头中识别到某个人我们需要检测在其余哪些摄像头中这个人再次出现其核心人物用到对图像特征的有效提取以及对图像特征之间的科学比对也就是说我们需要做到在多摄像头的复杂场景中快速定位查找指定目标的所有结果当然这只是我个人对行人重识别的理解其专业定义如下行人重识别是计算机视觉领域的一项关键技术旨在通过跨摄像头或跨时空的非重叠视域中对特定行人目标进行快速检索与匹配其核心任务可定义为给定一个查询行人图像从大规模候选图库中检索出同一行人的不同实例解决因视角变化遮挡光照差异姿态多样性等因素导致的视觉表观差异问题研究背景随着深度学习的发展传统的光学识别基于神经网络的图像识别等主流识别方式在各个领域取得了显著进展但它们在行人重识别任务中仍存在明显的局限性以图像分类和目标检测为例这些技术主要关注物体类别的判断和定位但缺乏对个体身份的细粒度区分能力例如等目标检测算法可以高效地框出监控画面中的行人位置却无法判断不同摄像头下的行人是否为同一人而等分类模型虽然能识别人这一类别却不能区分具体的行人人脸识别技术虽然在身份验证场景表现优异但在远距离低分辨率或遮挡情况下如行人背对摄像头或佩戴口罩几乎失效且面临严格的隐私合规限制对于我们上面提到的任务场景从摄像头的画面中提取某个特定的人像主流的识别方式可能面临下面的不足之处摄像头的覆盖面较广具体的人像信息需要从一张完整的图片中截取截取后势必需要放大核心图像这会导致得到的人像信息分辨率过低难以提取特征对于视角和姿势的变化处理能力不足难以应对遮挡现象对应分区的特征无法对应比如我们在比对两张图片的特征时其中一张图片对应位置被遮挡很可能造成特征比对失败在应对光照变化视觉模糊性等不利环境因素时的处理不够优秀跨域问题模型难以应用到更多场景中可能识别学校中的图像效果较好但识别街道超市等其他图像可能效果不佳常用数据集目前各大论文中常用的数据集都是以校园人物为主比较出名的数据集有香港中文大学校园清华大学校园个摄像机个室外和个室内校园这里我们采用第一个数据集进行实验该数据集中选取了个人像作为样本共有组摄像头进行拍摄论文中数据集的获取方式先通过摄像头拍摄全景照片接着从中扣取对应的人像作为样本在数据获取中不可避免会出现分辨率降低的问题二评估标准对于行人重识别我们常用的识别效果的评估标准一般有两个和标准识别的返回结果中会包含一系列的图像从前到后与目标图像的相似度依次降低指的是返回的第一张结果正确也就是观测相似度最高的那张图片是否识别正确标准需要计算多次输入的综合的结果因此我们需要先给出的概念对于一张待识别的图像我们返回了若干张识别的结果我们从前往后统计每一张正确的图像本应该是第几个返回的对于下面的例子第一个结果正确且第一个返回因此其得分为第二个结果错误直接跳过得分为第三个结果正确它是第二个正确的结果本应该在第二个返回但现在在第三个返回因此其得分为第四五个结果错误直接跳过得分为第六个结果正确它是第三个正确的结果本应该在第三个返回但现在在第个返回因此其得分为对于前个结果依次类推总得分为我们还要除以正确图片的数量也就是共有张正确图片最后这一组查询的值就是而则是对多次查询得到的值再次取平均得到的结果比如对于第二次查询我们得到的值就是显然值的理想结果为也就是每一张识别正确的图片都在其该在的位置上此时也就等价于返回的每一张图片都识别正确综合这两次查询得到的指标就是损失函数在行人重识别中我们用到的损失函数通常是分类损失所谓三元组损失需要准备三份数据假设一个集合中有三个人每个人有四张不同的图像那么该数据集可以表示为我们用表示当前数据比如表示和相同人的数据比如表示和不同人的数据比如经过网络处理以后分别处理成三个特征向量我们希望和对应的向量距离尽可能近同一个人和对应的向量距离尽可能远不同的人通过向量之间的差异挑中网络的权重参数具体表示公式其中表示经过网络进行编码后得到的特征向量这么处理的问题在于如果生成的特征向量每一维都是表达式也是成立的因此我们一般会把公式修改成引入一个参数这个通常叫做也就是间隔表示和至少要相差多少因为同类人像的距离肯定天生就比不同人像的距离要远拿这样一个性质去训练模型模型很难学到真正的特征因此我们需要加上额外的限定也就是不仅要让二者有距离差还要保证同类的距离要比不同类的距离至少小这么多实际训练中用到的也就是说只有不成立的时候才需要模型进行学习但实际上这个条件在很多情况下还是比较容易成立的那么模型多数时候都不需要学习训练效果就不会太好通常为解决这种问题我们使用方法也就是在选择样本的时候当和尽可能接近给模型带来一些挑战才能刺激它进行学习还是对于上面的集合而言假设我们选择作为那么和的选择就不能任选了首先我们预先计算其余图像和之间的距离其中要在中选和距离最大的要在中选和距离最小的因此才符合要求在多数论文的训练数据处理时为了保证网络模型能够保持较好的学习能力一般都会遵循要求进行三元组数据选择三核心算法这里我们选择复现代码的论文是该论文的主要创意是在行人重识别中加入了注意力机制该论文所实现的核心机制如下其创新点主要分为基于的注意力机制和基于的注意力机制对于传统神经网络以为例的图像特征提取而言图像经过网络时会被卷积层逐层处理提取特征此外不会再有其余处理在论文的处理中对于一张高度为宽度为有层的图像每个会从不同层面描述图像比如色彩层面纹理层面亮度层面等传统的剪枝策略是根据每一层的重要性决定其去留这里我们参考这一剪枝策略为不同的文中也称为特征图根据其重要性赋予不同的权重系数也就是说我们会生成一组权重向量其中每个都是范围的权重系数这一组权重系数即论文中所提到的也就是本文所用到的注意力机制基于的方法除了基于的权重系数选取本文还提出了基于空间的特征系数选取也就是其核心思路如下现在我们取其中一张特征图为例该特征图会被网络解析成若干个特征点我们以的特征图为例该特征图经过网络可能就会被解析成个特征点现在我们需要知道这些特征点哪些重要哪些不那么重要这些特征点的重要性比如展现人面部衣着的点相对重要而画面中的背景边界点没那么重要因此本文提出了基于特征点的位置不同计算其重要程度的思路因此需要为每一层的不同的特征点也赋予一组权重系数以的一层为例就需要为这个特征点依次准备个权重系数核心特征提取流程在具体训练与实现过程中我们需要借助卷积神经网络的卷积操作完成特征提取并对不同特征层与特征点赋予相应的权重对于一个高度为宽度为有个特征层的图像我们分别按照不同层以及每一层的不同特征点进行卷积处理如下图所示在接下来的表示中我们会使用的形式表示图像三个维度分别表示图像的高度宽度特征层数基于的机制我们以的一张图像为例首先其会分别经过和两个卷积层卷积得到的结果为卷积得到的结果为相较于会对结果矩阵做一次转置接着我们把这两个矩阵相乘得到的矩阵矩阵上的点表示原图像上某一层的第个点到第个点之间的关系接着我们把图像的第三维压缩与两次卷积的结果拼接最后做一次的卷积即可得到最终的结果基于的机制我们还是以的一张图像为例首先其会经过和两个卷积层只不过此时会对结果进行转置而处理得到的结果不变我们就会得到两次卷积的结果分别为和那么此时我们把这两个矩阵相乘得到的矩阵矩阵上的点表示原图像的第个特征层到第个特征层之间的关系接着我们把图像的前两维压缩与两次卷积的结果拼接最后做一次的卷积即可得到最终的结果值得一提的事参考论文中认为在特征矩阵中和的关系与和的关系之间并不等价而实验结果也表明在考虑不对称性的时候当成有向图处理模型的训练效果会明显变好以上就是参考论文中所使用的核心思路其整体框架采用的残差网络模型核心创新点在于与两处注意力机制的运用接下来我们将基于所提供的开源代码以残差网络作为主要框架建立相关模型并进行训练和测试完成对上述论文的整体效果复现四代码复现数据集选取选用数据集进行训练该数据集是香港中文大学发布的行人重识别数据集包含个行人共张图像采集自个摄像头其中每一张图片的文件名格式遵循如下规则字段说明示例值摄像头对编号行人从开始视角编号或图像序号值得一提的是这里我们并未沿用该数据集传统的划分协议做次随机训练测试划分每次划分个测试训练而是采用最新的划分协议即采用训练测试的固定划分很好地解决了经典协议的评测波动问题依据划分规则我们可以生成和两个图片集合分别用于模型训练和模型测试对于原始的数据集划分协议以及我们手动生成的数据划分脚本都已在附件的目录下一同上传超参数设置训练时参数参数缩写含义取值作用说明模型架构使用带有的变体批量大小每个的总数数据集使用的人工标注版本非自动检测版数据加载线程数表示在主进程加载数据避免多进程问题优化器使用优化器自适应学习率概率关闭所有层模型本身有正则化合并训练验证集将验证集并入训练集增大数据量随机种子固定随机数生成确保实验可复现数量使用单卡训练训练轮次总迭代次数包含特征维度最终特征向量的长度开始保存的轮次前轮不保存模型节省空间分支结构名使用分支空间通道关系建模数据集路径数据集根目录需含子目录日志路径保存日志模型评估结果的目录仅评估模式不训练直接加载模型测试模型恢复路径从第轮的检查点继续测试时参数参数类型作用取值示例注意事项标志位启用评估模式必须与配合使用会跳过训练直接测试模型路径参数指定检查点路径需确保文件存在包含完整模型结构和状态字典核心代码框架基于原有论文思路与框架我加入了对应的数据集一集中的文件最终得到的代码框架如下可视化图表空间注意力结构图通道注意力结构图数据集目录数据集来自原始文件自动检测版图像人工标注版图像划分协议预训练权重初始化权重基础权重训练保存的模型核心实现模型工厂注册等修改版支持插入核心实现数据工具数据集类定义图像预处理采样策略模型保存加载损失函数三元组损失分类损失训练脚本包含上述训练与测试的超参数主训练入口下面将通过展示核心代码片段讲解在模型训练过程中的关键处理流程完整代码框架将于附件中提交数据加载数据加载模块负责读取和解析行人重识别数据集如通过读取文件中的划分协议将图像路径行人和摄像头映射为字典并构建训练测试集索引支持两种数据版本人工标注框和检测器生成框确保数据格式统一便于后续处理从文件解析路径到字典图像预处理预处理流程对输入图像进行标准化增强包括尺寸归一化统一缩放到分辨率数据增强随机水平翻转概率归一化使用的均值和标准差进行标准化张量转换将图像转为张量通过封装确保训练和测试阶段的一致性统一分辨率权重加载逻辑模型支持两种权重初始化方式预训练权重加载预训练的权重作为骨干网络的初始参数断点续训从检查点恢复模型状态优化器状态和训练进度通过函数实现灵活加载支持从任意训练阶段恢复核心实现模块通过全局关系建模增强特征表达包含两个并行分支空间注意力计算所有空间位置的成对相关性生成空间权重通道注意力计算通道维度的互相关矩阵生成通道权重两者加权融合后通过激活对原始特征进行自适应校准核心操作通过卷积高效实现空间注意力分支通道注意力分支特征融合特征关系计算关系计算是的核心数学操作空间关系将特征图展平为通过矩阵乘法得到的亲和力矩阵通道关系转置特征维度为计算的通道相关性矩阵通过批处理矩阵乘法高效实现全局关系建模避免逐位置计算的复杂度模型分为训练阶段和测试评估阶段分别通过不同的超参数进行执行二者的执行流程分别如下训练阶段评估阶段五模型评估与展望结果评估我们所使用的模型在行人重识别任务上展现出了显著的性能提升根据论文中报告的结果在数据集上空间通道注意力模型达到了的准确率和的值这一结果显著优于当时的主流方法如和其评估结果如下从实际运行结果来看仅仅使用较小的训练参数如时模型的损失函数以及准确率已经到达较优的状态下图是小数据集下模型的表现结果而当使用相同的训练配置时复现结果与论文结果基本一致验证了模块的有效性经过具体的实验训练与评估我们得以证明的高效性特别是在处理遮挡和视角变化等复杂场景时通过全局关系建模能够更好地捕捉行人特征的判别性部分从而提升匹配准确率不过也注意到在等更大规模数据集上的性能优势相对较小这表明其对数据规模的适应性还有提升空间模型改进与后续展望展望未来模型有几个值得探索的改进方向首先可以尝试将与架构相结合利用其强大的全局建模能力来进一步增强特征表示目前的自注意力机制虽然有效但计算复杂度较高未来可以设计更轻量化的关系计算方式其次针对跨域泛化问题可以考虑在中引入领域自适应模块使其能够更好地适应不同摄像头分布的数据另外当前的主要关注静态图像关系未来可以扩展到视频行人重识别任务通过时序关系建模来利用运动线索最后在实际部署方面可以通过知识蒸馏等技术压缩模型规模使其更适合资源受限的边缘设备应用从长远来看行人重识别技术将向着更智能更自适应的方向发展作为一种通用的注意力机制其核心思想也可以迁移到其他视觉任务中未来工作可以探索在目标检测图像分割等任务中的应用潜力特别是在需要建模长距离依赖关系的场景中同时随着自监督学习的兴起如何将与无监督表征学习相结合也是一个比较有前景的的研究方向总的来说为注意力机制的设计提供了新思路其后续发展值得持续关注其中机制在其他领域的应用也值得我们不断探索真正做到触类旁通学以致用六参考文献核心参考与复现论文其余参考文献如下已按引文格式列出等等等等等等等等等等等等等等等等等等等等等写在最后的话本文是暑期课程的大作业这门课也是很毒瘤的经典自学课由于网上的数据论文等资料比较分散原论文作者公开的源码也存在环境不适配等问题笔者在这里进行统一的总结如果你对或感兴趣又或者你不幸选了这门课只想早点水完作业去做别的事都希望本文能让后来人少走一些弯路论文原文代码与数据集后续会上传到对应链接代码复现需要有环境在本地中即可运行中的依赖库最好一个一个安装不要一起安装可能会报玄学错误其中库运行时需要保证环境中只有一个不要同时存在版本和版本其中支持的库需要升级到版本以上在环境下安装第三方库的话如果失败就试试反之亦然格外需要注意升级或删库的时候旧依赖的残存文件最好也要删干净在运行代码之前最好先执行这段代码能够正确输出各个版本号以后再开始正常代码的运行最后感谢拉比同学提供的技术支持',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-08-04 23:28:09',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"></head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><img class="loading-img nolazyload" alt="加载头像" src="https://npm.elemecdn.com/anzhiyu-blog-static@1.0.4/img/avatar.jpg"/><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.getElementById('loading-box').classList.add("loaded");
  },
  initLoading: () => {
    document.getElementById('loading-box').classList.remove("loaded")
  }
}
window.addEventListener('load',()=> { preloader.endLoading() })
setTimeout(function(){preloader.endLoading();},10000)

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"/><script async="async" src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><a id="site-name" href="/" accesskey="h"><div class="title">MyBlog</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)" rel="external nofollow noreferrer">PAGE_NAME</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/categories/"><span> 分类</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/tags/"><span> 标签</span></a></div></div></div><div id="nav-right"><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" alt="微信" src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" alt="支付宝" src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> 最新评论</span></div><div class="aside-list"><span>正在加载中...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/Python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">Python与机器学习<sup>13</sup></a><a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 1.05rem;">算法<sup>13</sup></a></div></div><hr/></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/08/"><span class="card-archive-list-date">八月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">4</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/07/"><span class="card-archive-list-date">七月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">24</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);" rel="external nofollow noreferrer"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="post-meta-categories"><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-inbox post-meta-icon"></i><a class="post-meta-categories" href="/categories/Python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url">Python与机器学习</a></span><span class="article-meta tags"><a class="article-meta__tags" href="/tags/Python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Python与机器学习</span></a></span></div></div><h1 class="post-title" itemprop="name headline">行人重识别</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2025-08-04T00:40:54.984Z" title="发表于 2025-08-04 08:40:54">2025-08-04</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2025-08-04T15:28:09.208Z" title="更新于 2025-08-04 23:28:09">2025-08-04</time></span></div><div class="meta-secondline"><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为香波地群岛"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>香波地群岛</span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A233.jpg"></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container" itemscope itemtype="http://example.com/2025/08/04/%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%E5%AD%A6%E4%B9%A0/"><header><a class="post-meta-categories" href="/categories/Python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url">Python与机器学习</a><a href="/tags/Python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" tabindex="-1" itemprop="url">Python与机器学习</a><h1 id="CrawlerTitle" itemprop="name headline">行人重识别</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">Destiny</span><time itemprop="dateCreated datePublished" datetime="2025-08-04T00:40:54.984Z" title="发表于 2025-08-04 08:40:54">2025-08-04</time><time itemprop="dateCreated datePublished" datetime="2025-08-04T15:28:09.208Z" title="更新于 2025-08-04 23:28:09">2025-08-04</time></header><h1 id="行人重识别（基于Relation-Aware-Global-Attention论文学习与代码复现）"><a href="#行人重识别（基于Relation-Aware-Global-Attention论文学习与代码复现）" class="headerlink" title="行人重识别（基于Relation-Aware Global Attention论文学习与代码复现）"></a><strong>行人重识别（基于Relation-Aware Global Attention论文学习与代码复现）</strong></h1><h2 id="一、任务介绍"><a href="#一、任务介绍" class="headerlink" title="一、任务介绍"></a><strong>一、任务介绍</strong></h2><h3 id="1-1-什么是行人重识别"><a href="#1-1-什么是行人重识别" class="headerlink" title="1.1 什么是行人重识别"></a><strong>1.1 什么是行人重识别</strong></h3><p>在计算机视觉领域中，<strong>行人重识别</strong>主要指的是在当前摄像头中识别到某个人，我们需要检测在其余哪些摄像头中这个人再次出现，其核心人物用到对图像特征的有效提取，以及对图像特征之间的科学比对，也就是说我们需要做到在多摄像头的复杂场景中，快速定位查找指定目标的所有结果。</p>
<p>当然这只是我个人对行人重识别的理解，其专业定义如下，<strong>行人重识别（Person Re-identification，ReID）</strong> 是计算机视觉领域的一项关键技术，旨在通过跨摄像头或跨时空的非重叠视域中，对特定行人目标进行快速检索与匹配。其核心任务可定义为：</p>
<blockquote>
<p><strong>给定一个查询行人图像（query），从大规模候选图库（gallery）中检索出同一行人的不同实例</strong>，解决因视角变化、遮挡、光照差异、姿态多样性等因素导致的视觉表观差异问题。</p>
</blockquote>
<h3 id="1-2-研究背景"><a href="#1-2-研究背景" class="headerlink" title="1.2 研究背景"></a><strong>1.2 研究背景</strong></h3><p>随着深度学习的发展，传统的光学 OCR 识别，基于神经网络的图像识别等主流识别方式在各个领域取得了显著进展，但它们在行人重识别任务中仍存在明显的<strong>局限性</strong>。以图像分类和目标检测为例，这些技术主要关注物体类别的判断和定位，但缺乏对个体身份的细粒度区分能力。例如，YOLOv8等目标检测算法可以高效地框出监控画面中的行人位置，却无法判断不同摄像头下的行人是否为同一人；而ResNet等分类模型虽然能识别”人”这一类别，却不能区分具体的行人ID。人脸识别技术虽然在身份验证场景表现优异，但在远距离、低分辨率或遮挡情况下（如行人背对摄像头或佩戴口罩）几乎失效，且面临严格的隐私合规限制。</p>
<p>对于我们上面提到的任务场景，从摄像头的画面中提取某个特定的人像，主流的识别方式可能面临下面的不足之处：</p>
<ol>
<li>摄像头的覆盖面较广，具体的人像信息需要从一张完整的图片中截取，截取后势必需要放大核心图像，这会导致得到的人像信息分辨率过低，难以提取特征</li>
<li>对于视角和姿势的变化处理能力不足，难以应对遮挡现象（对应分区的特征无法对应），比如我们在比对两张图片的特征时，其中一张图片对应位置被遮挡，很可能造成特征比对失败</li>
<li>在应对光照变化，视觉模糊性等不利环境因素时的处理不够优秀</li>
<li>跨域问题：模型难以应用到更多场景中，可能识别学校中的图像效果较好，但识别街道、超市等其他图像可能效果不佳</li>
</ol>
<h3 id="1-3-常用数据集"><a href="#1-3-常用数据集" class="headerlink" title="1.3 常用数据集"></a><strong>1.3 常用数据集</strong></h3><p>目前各大论文中常用的数据集都是以校园人物（campus）为主，比较出名的数据集有：</p>
<ol>
<li>CUHK03：香港中文大学（CUHK）校园</li>
<li>Market-1501：清华大学校园</li>
<li>DUkeMTMC：8 个摄像机</li>
<li>MSMIT17：12 个室外和 3 个室内（校园）</li>
</ol>
<p>这里我们采用第一个数据集（CUHK03）进行实验：该数据集中选取了 1501 个人像作为样本，共有 6 组摄像头进行拍摄</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/..%5Cimages%5C%E6%95%B0%E6%8D%AE%E9%9B%861.png" alt="image-20250803213255786"></p>
<p>论文中数据集的获取方式：先通过摄像头拍摄全景照片，接着从中扣取对应的人像作为样本，在数据获取中不可避免会出现分辨率降低的问题</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/..%5Cimages%5C%E8%8E%B7%E5%8F%96%E6%96%B9%E5%BC%8F.png" alt="image-20250803213535353"></p>
<h2 id="二、评估标准"><a href="#二、评估标准" class="headerlink" title="二、评估标准"></a><strong>二、评估标准</strong></h2><p>对于行人重识别，我们常用的识别效果的评估标准一般有两个：rank1 和 map</p>
<h3 id="2-1-rank1标准"><a href="#2-1-rank1标准" class="headerlink" title="2.1 rank1标准"></a><strong>2.1 rank1标准</strong></h3><p>rank1：识别的返回结果中会包含一系列的图像，从前到后与目标图像的相似度依次降低，rank1 指的是返回的第一张结果正确（也就是观测相似度最高的那张图片是否识别正确）</p>
<h3 id="2-2-map标准"><a href="#2-2-map标准" class="headerlink" title="2.2 map标准"></a><strong>2.2 map标准</strong></h3><p>map：map（mean average precision）需要计算多次输入的综合 ap（average presicion）的结果，因此我们需要先给出 ap 的概念</p>
<p>对于一张待识别的图像，我们返回了若干张识别的结果，我们从前往后，统计<strong>每一张正确的图像本应该是第几个返回的</strong>，对于下面的例子：</p>
<ol>
<li>第一个结果正确，且第一个返回，因此其得分为 1&#x2F;1</li>
<li>第二个结果错误，直接跳过，得分为 0</li>
<li>第三个结果正确，它是第二个正确的结果，本应该在第二个返回，但现在在第三个返回，因此其得分为 2&#x2F;3</li>
<li>第四、五个结果错误，直接跳过，得分为 0</li>
<li>第六个结果正确，它是第三个正确的结果，本应该在第三个返回，但现在在第 6 个返回，因此其得分为 3&#x2F;6</li>
</ol>
<p>对于前 10 个结果依次类推，总得分为 (1 + 2&#x2F;3 + 3&#x2F;6 + 4&#x2F;9 + 5&#x2F;10)，我们还要除以<strong>正确图片的数量</strong>，也就是共有 5 张正确图片，最后这一组查询的 ap 值就是 (1 + 2&#x2F;3 + 3&#x2F;6 + 4&#x2F;9 + 5&#x2F;10) &#x2F; 5 &#x3D; 0.62</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/..%5Cimages%5Cap.png" alt="image-20250803214150826"></p>
<p>而 map 则是对多次查询得到的 ap 值<strong>再次取平均</strong>得到的结果，比如对于第二次查询，我们得到的 ap 值就是 (1&#x2F;2 + 2&#x2F;5 + 3&#x2F;7) &#x2F; 3 - 0.44</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/..%5Cimages%5C%E7%AC%AC%E4%BA%8C%E6%AC%A1%E6%9F%A5%E8%AF%A2.png" alt="image-20250803214740898"></p>
<p>显然，ap 值的理想结果为 1，也就是每一张识别正确的图片都在其该在的位置上，此时也就等价于返回的每一张图片都识别正确</p>
<p>综合这两次查询，得到的 map 指标就是 0.62 + 0.44 &gt;&gt; 1 &#x3D; 0.53</p>
<h3 id="2-3-损失函数"><a href="#2-3-损失函数" class="headerlink" title="2.3 损失函数"></a><strong>2.3 损失函数</strong></h3><p>在行人重识别中，我们用到的损失函数，通常是分类损失 + Triplet loss</p>
<p>所谓 Triplet loss（三元组损失），需要准备三份数据，假设一个集合中有 A B C 三个人，每个人有四张不同的图像，那么该数据集可以表示为 S &#x3D; {A1,A2,A3,A4,B1,B2,B3,B4,C1,C2,C3,C4}，我们用 Anchor 表示当前数据（比如 A1），Positive 表示和 A 相同人的数据（比如 A3），Negative 表示和 A 不同人的数据（比如 C2）</p>
<p>经过网络处理以后，Anchor，Positive，Negative 分别处理成三个特征向量，我们希望 Anchor 和 Positive 对应的向量距离尽可能近（同一个人），Anchor 和 Negative 对应的向量距离尽可能远（不同的人），通过向量之间的差异挑中网络的权重参数</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/image-20250804232733917.png" alt="image-20250804232733917"></p>
<p>Triplet loss 具体表示公式：|| f(A) - f(P) ||^2^ &lt;&#x3D; || f(A) - f(N) ||^2^，其中 f 表示经过网络进行编码后得到的特征向量</p>
<p>这么处理的问题在于，如果 f 生成的特征向量每一维都是 0，表达式也是成立的，因此我们一般会把公式修改成：|| f(A) - f(P) ||^2^ - || f(A) - f(N) ||^2^ + a &lt;&#x3D; 0，引入一个参数 a，这个 a 通常叫做 margin，也就是<strong>间隔</strong>，表示 d(A, P) 和 d(A, N) 至少要相差多少</p>
<p>因为同类人像的距离肯定天生就比不同人像的距离要远，拿这样一个性质去训练模型，模型很难学到真正的特征，因此我们需要加上额外的限定，也就是不仅要让二者有距离差，还要保证同类的距离要比不同类的距离至少小 a 这么多</p>
<p>实际训练中用到的 Triplet loss &#x3D; max(0, || f(A) - f(P) ||^2^ - || f(A) - f(N) ||^2^ + a)，也就是说，只有 || f(A) - f(P) ||^2^ - || f(A) - f(N) ||^2^ + a &lt;&#x3D; 0 不成立的时候，才需要模型进行学习</p>
<p>但实际上，|| f(A) - f(P) ||^2^ + a &lt;&#x3D; || f(A) - f(N) ||^2^ 这个条件在很多情况下还是比较容易成立的，那么模型多数时候都不需要学习，训练效果就不会太好，通常为解决这种问题，我们使用 <strong>hard negative</strong> 方法，也就是在选择样本的时候，当 d(A, P) 和 d(A, N) 尽可能接近，给模型带来一些<strong>挑战</strong>，才能刺激它进行学习</p>
<p>还是对于上面的集合 S &#x3D; {A1,A2,A3,A4,B1,B2,B3,B4,C1,C2,C3,C4} 而言，假设我们选择 A1 作为 Anchor，那么 Positive 和 Negative 的选择就不能任选了，首先我们预先计算其余图像和 A1 之间的距离，其中 Positive 要在 A2 ~ A4 中选和 A1 距离最大的，Negative 要在 B1 ~ C4 中选和 A1 距离最小的，因此才符合 hard negative 要求</p>
<p>在多数论文的训练数据处理时，为了保证网络模型能够保持较好的学习能力，一般都会遵循 hard negative 要求进行三元组数据选择  </p>
<h2 id="三、核心算法"><a href="#三、核心算法" class="headerlink" title="三、核心算法"></a><strong>三、核心算法</strong></h2><p>这里我们选择复现代码的论文是《Relation-Aware Global Attention》，该论文的主要创意是在行人重识别中加入了<strong>注意力</strong>机制，该论文所实现的核心机制如下（spatial_channel_RGA），其创新点主要分为**基于 channel <strong>的注意力机制和</strong>基于 space **的注意力机制：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/spatial_channel_RGA.png" alt="spatial_channel_RGA"></p>
<h3 id="3-1-channel-attention"><a href="#3-1-channel-attention" class="headerlink" title="3.1 channel attention"></a><strong>3.1 channel attention</strong></h3><p>对于传统神经网络（以 resnet50 为例）的图像特征提取而言，图像经过网络时，会被卷积层逐层处理提取特征，此外不会再有其余处理</p>
<p>在论文的处理中，对于一张高度为 h，宽度为 w，有 256 层 channel 的图像（每个 channel 会从不同层面描述图像，比如色彩层面，纹理层面，亮度层面等……），传统的剪枝策略是根据每一层 channel 的重要性决定其去留，这里我们参考这一剪枝策略，为不同的 channel（文中也称为特征图），根据其重要性，赋予不同的权重系数，也就是说，我们会生成一组权重向量 w &#x3D; {w1, w2, w3,……，w256}，其中每个 w 都是 (0, 1) 范围的权重系数</p>
<p>这一组权重系数即论文中所提到的<code>Channel Relation-Aware Global Attention</code>，也就是本文所用到的注意力机制（基于 channel 的 attention 方法）</p>
<h3 id="3-2-spatial-attention"><a href="#3-2-spatial-attention" class="headerlink" title="3.2 spatial attention"></a><strong>3.2 spatial attention</strong></h3><p>除了基于 channel 的权重系数选取，本文还提出了基于空间的特征系数选取，也就是<code>Spatial Relation-Aware Global Attention</code>，其核心思路如下：</p>
<p>现在我们取其中一张特征图为例，该特征图会被网络解析成若干个特征点，我们以 h &#x3D; 32， w &#x3D; 32 的特征图为例，该特征图经过网络可能就会被解析成 32 × 32 个特征点，现在我们需要知道，这些特征点哪些重要，哪些不那么重要这些特征点的重要性（比如展现人面部，衣着的点相对重要，而画面中的背景，边界点没那么重要），因此本文提出了基于特征点的位置不同，计算其重要程度的思路，因此需要为每一层的不同的特征点也赋予一组权重系数，以 32 × 32 的一层为例，就需要为这 1024 个特征点依次准备 w &#x3D; {w1, w2, ……,w1024} 个权重系数</p>
<h3 id="3-3-核心特征提取流程"><a href="#3-3-核心特征提取流程" class="headerlink" title="3.3 核心特征提取流程"></a><strong>3.3 核心特征提取流程</strong></h3><p>在具体训练与实现过程中，我们需要借助卷积神经网络（CNN）的卷积操作，完成特征提取并对不同特征层与特征点赋予相应的权重</p>
<p>对于一个高度为 h，宽度为 w，有 C 个特征层的图像，我们分别按照不同层（b），以及每一层的不同特征点（a）进行卷积处理，如下图所示：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/spatial_channel_RGA.png" alt="spatial_channel_RGA"></p>
<p>在接下来的表示中，我们会使用 [a, b, c] 的形式表示图像，三个维度分别表示图像的高度，宽度，特征层数</p>
<p>（a）基于<code>space</code>的<code>attention</code>机制：我们以 [32, 64, 256] 的一张图像为例，首先其会分别经过 F1 和 F2 两个卷积层，F1 卷积得到的结果为 [2048, 256]，F2 卷积得到的结果为 [256, 2048]（相较于 F1 会对结果矩阵做一次转置），接着我们把这两个矩阵相乘，得到 [2048, 2048] 的矩阵，矩阵上的点 (i, j) 表示原图像上某一层的第 i 个点，到第 j 个点之间的关系，接着我们把图像的第三维压缩，与两次卷积的结果拼接，最后做一次 1×1 的卷积，即可得到最终<code>Apatial Attention</code>的结果</p>
<p>（b）基于<code>channel</code>的<code>attention</code>机制：我们还是以 [32, 64, 256] 的一张图像为例，首先其会经过 F1 和 F2 两个卷积层，只不过此时 F1 会对结果进行转置，而 F2 处理得到的结果不变，我们就会得到两次卷积的结果分别为 [256, 2048] 和 [2048, 256]，那么此时我们把这两个矩阵相乘，得到 [256, 256] 的矩阵，矩阵上的点 (i, j) 表示原图像的第 i 个特征层，到第 j 个特征层之间的关系，接着我们把图像的前两维压缩，与两次卷积的结果拼接，最后做一次 1×1 的卷积，即可得到最终<code>Channel Attention</code>的结果</p>
<p>值得一提的事，参考论文中认为，在特征矩阵中，i 和 j 的关系，与 j 和 i 的关系之间并不等价，而实验结果也表明，在考虑不对称性的时候（当成有向图处理），模型的训练效果会明显变好</p>
<p>以上就是参考论文中所使用的核心思路，其整体框架采用<code>resnet50</code>的残差网络模型，核心创新点在于<code>Channel Relation-Aware Global Attention</code>与<code>Spatial Relation-Aware Global Attention</code>两处注意力机制的运用，接下来，我们将基于<code>Microsoft/Relation-Aware Global Attention</code>所提供的开源代码，以<code>resnet50</code>残差网络作为主要框架，建立相关模型并进行训练和测试，完成对上述论文的整体效果复现</p>
<h2 id="四、代码复现"><a href="#四、代码复现" class="headerlink" title="四、代码复现"></a><strong>四、代码复现</strong></h2><h3 id="4-1-数据集选取"><a href="#4-1-数据集选取" class="headerlink" title="4.1 数据集选取"></a><strong>4.1 数据集选取</strong></h3><p>选用 <strong>CUHK03 数据集</strong>进行训练，该数据集是香港中文大学发布的行人重识别数据集，包含 1,467 个行人ID，共  13,164 张图像，采集自 6 个摄像头</p>
<p>其中每一张图片的文件名格式遵循如下规则：<br><code>&#123;campair_id&#125;_&#123;pid&#125;_&#123;view_id&#125;_&#123;img_id&#125;.png</code></p>
<table>
<thead>
<tr>
<th align="center">字段</th>
<th align="center">说明</th>
<th align="center">示例值</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>campair_id</code></td>
<td align="center">摄像头对编号 (1-5)</td>
<td align="center"><code>1</code></td>
</tr>
<tr>
<td align="center"><code>pid</code></td>
<td align="center">行人ID (从1开始)</td>
<td align="center"><code>001</code></td>
</tr>
<tr>
<td align="center"><code>view_id</code></td>
<td align="center">视角编号 (1或2)</td>
<td align="center"><code>1</code></td>
</tr>
<tr>
<td align="center"><code>img_id</code></td>
<td align="center">图像序号 (01-10)</td>
<td align="center"><code>01</code></td>
</tr>
</tbody></table>
<p>值得一提的是，这里我们并未沿用该数据集传统的划分协议（做 20 次随机训练 &#x2F; 测试划分，每次划分：100 个测试 ID + 训练 ID），而是采用最新的划分协议，即采用 767 训练 ID + 700 测试 ID 的固定划分，很好地解决了经典协议的评测波动问题</p>
<p>依据划分规则，我们可以生成<code>images_labeled</code>和<code>images_detected</code>两个图片集合，分别用于模型训练和模型测试，对于原始的 CUHK03 数据集，划分协议，以及我们手动生成的数据划分脚本，都已在附件的 data 目录下一同上传</p>
<h3 id="4-2-超参数设置"><a href="#4-2-超参数设置" class="headerlink" title="4.2 超参数设置"></a><strong>4.2 超参数设置</strong></h3><p>训练时参数：</p>
<table>
<thead>
<tr>
<th align="center">参数</th>
<th align="center">缩写</th>
<th align="center">含义</th>
<th align="center">取值</th>
<th align="center">作用说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>--arch</code></td>
<td align="center"><code>-a</code></td>
<td align="center">模型架构</td>
<td align="center"><code>resnet50_rga</code></td>
<td align="center">使用带有Relation-Aware Global Attention的ResNet50变体</td>
</tr>
<tr>
<td align="center"><code>--batch-size</code></td>
<td align="center"><code>-b</code></td>
<td align="center">批量大小</td>
<td align="center"><code>16</code></td>
<td align="center">每个GPU的batch size（总batch&#x3D;16*GPU数）</td>
</tr>
<tr>
<td align="center"><code>--dataset</code></td>
<td align="center"><code>-d</code></td>
<td align="center">数据集</td>
<td align="center"><code>cuhk03labeled</code></td>
<td align="center">使用CUHK03的人工标注版本（非自动检测版）</td>
</tr>
<tr>
<td align="center"><code>--workers</code></td>
<td align="center"><code>-j</code></td>
<td align="center">数据加载线程数</td>
<td align="center"><code>0</code></td>
<td align="center">0表示在主进程加载数据（避免多进程问题）</td>
</tr>
<tr>
<td align="center"><code>--optimizer</code></td>
<td align="center"><code>--opt</code></td>
<td align="center">优化器</td>
<td align="center"><code>adam</code></td>
<td align="center">使用Adam优化器（自适应学习率）</td>
</tr>
<tr>
<td align="center"><code>--dropout</code></td>
<td align="center">-</td>
<td align="center">Dropout概率</td>
<td align="center"><code>0</code></td>
<td align="center">关闭所有dropout层（模型本身有RGA正则化）</td>
</tr>
<tr>
<td align="center"><code>--combine-trainval</code></td>
<td align="center">-</td>
<td align="center">合并训练验证集</td>
<td align="center"><code>True</code></td>
<td align="center">将验证集并入训练集增大数据量</td>
</tr>
<tr>
<td align="center"><code>--seed</code></td>
<td align="center">-</td>
<td align="center">随机种子</td>
<td align="center"><code>16</code></td>
<td align="center">固定随机数生成（确保实验可复现）</td>
</tr>
<tr>
<td align="center"><code>--num-gpu</code></td>
<td align="center">-</td>
<td align="center">GPU数量</td>
<td align="center"><code>1</code></td>
<td align="center">使用单卡训练</td>
</tr>
<tr>
<td align="center"><code>--epochs</code></td>
<td align="center">-</td>
<td align="center">训练轮次</td>
<td align="center"><code>600</code></td>
<td align="center">总迭代次数（包含warmup）</td>
</tr>
<tr>
<td align="center"><code>--features</code></td>
<td align="center">-</td>
<td align="center">特征维度</td>
<td align="center"><code>2048</code></td>
<td align="center">最终特征向量的长度</td>
</tr>
<tr>
<td align="center"><code>--start-save</code></td>
<td align="center">-</td>
<td align="center">开始保存的轮次</td>
<td align="center"><code>300</code></td>
<td align="center">前300轮不保存模型（节省空间）</td>
</tr>
<tr>
<td align="center"><code>--branch-name</code></td>
<td align="center">-</td>
<td align="center">分支结构名</td>
<td align="center"><code>rgasc</code></td>
<td align="center">使用RGA-SC分支（空间+通道关系建模）</td>
</tr>
<tr>
<td align="center"><code>--data-dir</code></td>
<td align="center">-</td>
<td align="center">数据集路径</td>
<td align="center"><code>./data</code></td>
<td align="center">数据集根目录（需含<code>cuhk03</code>子目录）</td>
</tr>
<tr>
<td align="center"><code>--logs-dir</code></td>
<td align="center">-</td>
<td align="center">日志路径</td>
<td align="center"><code>./logs/RGA-SC/...</code></td>
<td align="center">保存日志&#x2F;模型&#x2F;评估结果的目录</td>
</tr>
<tr>
<td align="center"><code>--evaluate</code></td>
<td align="center">-</td>
<td align="center">仅评估模式</td>
<td align="center"><code>True</code></td>
<td align="center">不训练，直接加载模型测试</td>
</tr>
<tr>
<td align="center"><code>--resume</code></td>
<td align="center">-</td>
<td align="center">模型恢复路径</td>
<td align="center"><code>./logs/.../checkpoint_30.pth.tar</code></td>
<td align="center">从第30轮的检查点继续</td>
</tr>
</tbody></table>
<p>测试时参数：</p>
<table>
<thead>
<tr>
<th align="center">参数</th>
<th align="center">类型</th>
<th align="center">作用</th>
<th align="center">取值示例</th>
<th align="center">注意事项</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>--evaluate</code></td>
<td align="center">标志位</td>
<td align="center">启用评估模式</td>
<td align="center"><code>True</code>&#x2F;<code>False</code></td>
<td align="center">1. 必须与<code>--resume</code>配合使用 2. 会跳过训练直接测试模型</td>
</tr>
<tr>
<td align="center"><code>--resume</code></td>
<td align="center">路径参数</td>
<td align="center">指定检查点路径</td>
<td align="center"><code>./path/to/checkpoint_xx.pth.tar</code></td>
<td align="center">1. 需确保文件存在 2. 包含完整模型结构和状态字典</td>
</tr>
</tbody></table>
<h3 id="4-3-核心代码框架"><a href="#4-3-核心代码框架" class="headerlink" title="4.3 核心代码框架"></a><strong>4.3 核心代码框架</strong></h3><p>基于原有论文思路与框架，我加入了对应的数据集一集 pytorch 中的<code>resnet50-19c8e357.pth</code>文件，最终得到的代码框架如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── diagrams/                      <span class="comment"># 可视化图表</span></span><br><span class="line">│   ├── RGA-S.png                  <span class="comment"># 空间注意力结构图(Spatial)</span></span><br><span class="line">│   └── RGA-C.png                  <span class="comment"># 通道注意力结构图(Channel)</span></span><br><span class="line">├── data/                           <span class="comment"># 数据集目录</span></span><br><span class="line">│   ├── cuhk03/                    <span class="comment"># CUHK03数据集(来自原始 mat 文件)</span></span><br><span class="line">│   │   ├── images_detected/       <span class="comment"># 自动检测版图像</span></span><br><span class="line">│   │   ├── images_labeled/        <span class="comment"># 人工标注版图像</span></span><br><span class="line">│   │   └── splits/                <span class="comment"># 划分协议</span></span><br><span class="line">├── weights/                       <span class="comment"># 预训练权重</span></span><br><span class="line">│   ├── pre_train/                 <span class="comment"># 初始化权重</span></span><br><span class="line">│   │   └── resnet50-19c8e357.pth  <span class="comment"># ResNet50 基础权重</span></span><br><span class="line">│   └── trained/                   <span class="comment"># 训练保存的模型</span></span><br><span class="line">├── reid/                          <span class="comment"># ReID 核心实现</span></span><br><span class="line">│   ├── models/</span><br><span class="line">│   │   ├── __init__.py           <span class="comment"># 模型工厂(注册resnet50_rga等)</span></span><br><span class="line">│   │   ├── resnet.py             <span class="comment"># 修改版ResNet50(支持RGA插入)</span></span><br><span class="line">│   │   └── rga_modules.py        <span class="comment"># RGA核心实现</span></span><br><span class="line">│   ├── utils/</span><br><span class="line">│   │   ├── data/                 <span class="comment"># 数据工具</span></span><br><span class="line">│   │   │   ├── datasets/        <span class="comment"># 数据集类定义</span></span><br><span class="line">│   │   │   ├── preprocessor.py  <span class="comment"># 图像预处理</span></span><br><span class="line">│   │   │   └── samplers/       <span class="comment"># 采样策略</span></span><br><span class="line">│   │   └── serialization.py     <span class="comment"># 模型保存/加载</span></span><br><span class="line">│   └── loss/                     <span class="comment"># 损失函数</span></span><br><span class="line">│       ├── triplet.py           <span class="comment"># 三元组损失</span></span><br><span class="line">│       └── cross_entropy.py     <span class="comment"># 分类损失</span></span><br><span class="line">├── scripts/</span><br><span class="line">│   └── run_rgasc_cuhk03.sh      <span class="comment"># 训练脚本(包含上述训练与测试的超参数)</span></span><br><span class="line">└── main_imgreid.py              <span class="comment"># 主训练入口</span></span><br></pre></td></tr></table></figure>

<p>下面将通过展示核心代码片段，讲解在模型训练过程中的关键处理流程，完整代码框架将于附件中提交</p>
<p>（1）数据加载(<code>reid/utils/data/datasets/cuhk03.py</code>)</p>
<p>数据加载模块负责读取和解析行人重识别数据集（如CUHK03），通过读取<code>.mat</code>文件中的划分协议，将图像路径、行人ID（PID）和摄像头ID（CamID）映射为Python字典，并构建训练&#x2F;测试集索引，支持两种数据版本：人工标注框（labeled）和检测器生成框（detected），确保数据格式统一，便于后续处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CUHK03</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_process_data</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 从.mat文件解析路径到Python字典</span></span><br><span class="line">        <span class="variable language_">self</span>.train = [(fname, pid, camid), ...]</span><br></pre></td></tr></table></figure>

<p>（2）图像预处理 (<code>reid/utils/data/preprocessor.py</code>)</p>
<p>预处理流程对输入图像进行标准化增强，包括：</p>
<ul>
<li>尺寸归一化：统一缩放到<code>256×128</code>分辨率</li>
<li>数据增强：随机水平翻转（概率50%）</li>
<li>归一化：使用ImageNet的均值和标准差进行标准化</li>
<li>张量转换：将图像转为PyTorch张量，通过<code>torchvision.transforms.Compose</code>封装，确保训练和测试阶段的一致性</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">transform = T.Compose([</span><br><span class="line">    T.RectScale(<span class="number">256</span>, <span class="number">128</span>),       <span class="comment"># 统一分辨率</span></span><br><span class="line">    T.RandomHorizontalFlip(p=<span class="number">0.5</span>),</span><br><span class="line">    T.ToTensor(),</span><br><span class="line">    T.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<p>（3）权重加载逻辑 (<code>main_imgreid.py</code>)</p>
<p>模型支持两种权重初始化方式：</p>
<ul>
<li>预训练权重：加载ImageNet预训练的ResNet50权重（<code>resnet50-19c8e357.pth</code>），作为骨干网络的初始参数</li>
<li>断点续训：从检查点（checkpoint）恢复模型状态、优化器状态和训练进度<br>通过<code>load_checkpoint()</code>函数实现灵活加载，支持从任意训练阶段恢复。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.resume:</span><br><span class="line">    checkpoint = load_checkpoint(args.resume)</span><br><span class="line">    model.load_state_dict(checkpoint[<span class="string">&#x27;state_dict&#x27;</span>]) </span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    init_pretrained_weights(model, <span class="string">&#x27;weights/pre_train/resnet50-19c8e357.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>（4）RGA核心实现（<code>reid/models/rga_modules.py</code>）</p>
<p>RGA模块通过全局关系建模增强特征表达，包含两个并行分支：</p>
<ul>
<li>空间注意力：计算所有空间位置（H×W）的成对相关性，生成空间权重</li>
<li>通道注意力：计算通道维度（C×C）的互相关矩阵，生成通道权重<br>两者加权融合后通过Sigmoid激活，对原始特征进行自适应校准。核心操作通过1×1卷积高效实现</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RGA_Module</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 空间注意力分支</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_spatial:</span><br><span class="line">            s_relation = <span class="variable language_">self</span>._get_spatial_relation(x)  <span class="comment"># [B, H*W, H*W]</span></span><br><span class="line">            s_attention = <span class="variable language_">self</span>.spatial_conv(s_relation)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 通道注意力分支  </span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_channel:</span><br><span class="line">            c_relation = <span class="variable language_">self</span>._get_channel_relation(x)  <span class="comment"># [B, C, C]</span></span><br><span class="line">            c_attention = <span class="variable language_">self</span>.channel_conv(c_relation)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 特征融合</span></span><br><span class="line">        <span class="keyword">return</span> x * torch.sigmoid(s_attention + c_attention)</span><br></pre></td></tr></table></figure>

<p>（5）特征关系计算</p>
<p>关系计算是 RGA 的核心数学操作：</p>
<ul>
<li>空间关系：将特征图展平为<code>[B, C, HW]</code>，通过矩阵乘法得到<code>[B, HW, HW]</code>的亲和力矩阵</li>
<li>通道关系：转置特征维度为<code>[B, HW, C]</code>，计算<code>[B, C, C]</code>的通道相关性矩阵<br>通过批处理矩阵乘法（<code>torch.bmm</code>）高效实现全局关系建模，避免逐位置计算的复杂度</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_get_spatial_relation</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="comment"># x: [B, C, H, W]</span></span><br><span class="line">    B, C, H, W = x.size()</span><br><span class="line">    x_flat = x.view(B, C, -<span class="number">1</span>)  <span class="comment"># [B, C, HW]</span></span><br><span class="line">    <span class="keyword">return</span> torch.bmm(x_flat.transpose(<span class="number">1</span>,<span class="number">2</span>), x_flat)  <span class="comment"># [B, HW, HW]</span></span><br></pre></td></tr></table></figure>

<p>模型分为训练阶段和测试评估阶段，分别通过不同的超参数进行执行，二者的执行流程分别如下</p>
<p>训练阶段：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5.png" alt="image-20250804223257821"></p>
<p>评估阶段：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E8%AF%84%E4%BC%B0%E9%98%B6%E6%AE%B5.png" alt="image-20250804223322945"></p>
<h2 id="五、模型评估与展望"><a href="#五、模型评估与展望" class="headerlink" title="五、模型评估与展望"></a><strong>五、模型评估与展望</strong></h2><h3 id="5-1-结果评估"><a href="#5-1-结果评估" class="headerlink" title="5.1 结果评估"></a><strong>5.1 结果评估</strong></h3><p>我们所使用的<code>Relation-Aware Global Attention Networks</code> (RGA) 模型在行人重识别任务上展现出了显著的性能提升，根据论文中报告的结果，在 CUHK03 数据集上，RGA-SC（空间+通道注意力）模型达到了76.1%的 Rank-1 准确率和 74.9% 的 map 值，这一结果显著优于当时的主流方法如 PCB 和 MGN，其评估结果如下：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E8%AF%84%E4%BC%B0%E7%BB%93%E6%9E%9C.png" alt="image-20250804230133895"></p>
<p>从实际运行结果来看，仅仅使用较小的训练参数（如 batch size &#x3D; 24，epochs &#x3D; 30）时，模型的损失函数以及准确率已经到达较优的状态，下图是小数据集下模型的表现结果，而当使用相同的训练配置（batch size&#x3D;64，epochs&#x3D;600）时，复现结果与论文结果基本一致，验证了RGA模块的有效性</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E7%BB%93%E6%9E%9C.png" alt="image-20250804224000737"></p>
<p>经过具体的实验训练与评估，我们得以证明 RGA 的高效性，特别是在处理遮挡和视角变化等复杂场景时，RGA 通过全局关系建模能够更好地捕捉行人特征的判别性部分，从而提升匹配准确率，不过也注意到，在Market-1501等更大规模数据集上，RGA 的性能优势相对较小，这表明其对数据规模的适应性还有提升空间</p>
<h3 id="5-2-模型改进与后续展望"><a href="#5-2-模型改进与后续展望" class="headerlink" title="5.2 模型改进与后续展望"></a><strong>5.2 模型改进与后续展望</strong></h3><p>展望未来，RGA模型有几个值得探索的改进方向，首先，可以尝试将 RGA 与 Transformer 架构相结合，利用其强大的全局建模能力来进一步增强特征表示，目前的自注意力机制虽然有效，但计算复杂度较高，未来可以设计更轻量化的关系计算方式，其次，针对跨域泛化问题，可以考虑在 RGA 中引入领域自适应模块，使其能够更好地适应不同摄像头分布的数据，另外，当前的 RGA 主要关注静态图像关系，未来可以扩展到视频行人重识别任务，通过时序关系建模来利用运动线索，最后，在实际部署方面，可以通过知识蒸馏等技术压缩模型规模，使其更适合资源受限的边缘设备应用</p>
<p>从长远来看，行人重识别技术将向着更智能、更自适应的方向发展，RGA 作为一种通用的注意力机制，其核心思想也可以迁移到其他视觉任务中。未来工作可以探索 RGA 在目标检测、图像分割等任务中的应用潜力，特别是在需要建模长距离依赖关系的场景中，同时，随着自监督学习的兴起，如何将 RGA 与无监督表征学习相结合也是一个比较有前景的的研究方向，总的来说，RGA 为注意力机制的设计提供了新思路，其后续发展值得持续关注，其中 Attention 机制在其他领域的应用，也值得我们不断探索，真正做到触类旁通，学以致用</p>
<h2 id="六、参考文献"><a href="#六、参考文献" class="headerlink" title="六、参考文献"></a><strong>六、参考文献</strong></h2><p>核心参考与复现论文：Relation-Aware Global Attention</p>
<p>其余参考文献如下（已按GB&#x2F;T 7714 引文格式列出）：</p>
<p>[1] ALMAZAN J, GAJIC B, MURRAY N, 等. Re-id done right: towards good practices for person re-identification[J]. arXiv preprint arXiv:1801.05339, 2018.<br>[2] BA J, MNIH V, KAVUKCUOGLU K. Multiple object recognition with visual attention[C]&#x2F;&#x2F;ICLR, 2014.<br>[3] BAHDANAU D, CHO K, BENGIO Y. Neural machine translation by jointly learning to align and translate[J]. arXiv preprint arXiv:1409.0473, 2014.<br>[4] BAI X, YANG M, HUANG T, 等. Deep-person: Learning discriminative deep features for person re-identification[J]. arXiv preprint arXiv:1711.10658, 2017.<br>[5] BARNES C, SHECHTMAN E, FINKELSTEIN A, 等. Patchmatch: A randomized correspondence algorithm for structural image editing[J]. ACM Transactions on Graphics, 2009, 28(3): 24.<br>[6] BUADES A, COLL B, MOREL J M. A non-local algorithm for image denoising[C]&#x2F;&#x2F;CVPR, 2005, 2: 60-65.<br>[7] BUADES A, COLL B, MOREL J M. Non-local color image denoising with convolutional neural networks[C]&#x2F;&#x2F;CVPR, 2017.<br>[8] CHEN L, ZHANG H, XIAO J, 等. Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning[C]&#x2F;&#x2F;CVPR, 2017: 5659-5667.<br>[9] CHEN L C, PAPANDREOU G, SCHROFF F, 等. Rethinking atrous convolution for semantic image segmentation[J]. arXiv preprint arXiv:1706.05587, 2017.<br>[10] CHOROWSKI J K, BAHDANAU D, SERDYUK D, 等. Attention-based models for speech recognition[C]&#x2F;&#x2F;NeurIPS, 2015: 577-585.<br>[11] CORBETTA M, SHULMAN G L. Control of goal-directed and stimulus-driven attention in the brain[J]. Nature reviews neuroscience, 2002, 3(3): 201.<br>[12] CORDTS M, OMRAN M, RAMOS S, 等. The cityscapes dataset for semantic urban scene understanding[C]&#x2F;&#x2F;CVPR, 2016: 3213-3223.<br>[13] DABOV K, FOI A, KATKOVNIK V, 等. Image denoising by sparse 3-d transform-domain collaborative filtering[J]. TIP, 2007, 16(8): 2080-2095.<br>[14] DENG J, DONG W, SOCHER R, 等. Imagenet: A large-scale hierarchical image database[C]&#x2F;&#x2F;CVPR, 2009.<br>[15] EFROS A A, LEUNG T K. Texture synthesis by non-parametric sampling[C]&#x2F;&#x2F;ICCV, 1999, 2: 1033-1038.<br>[16] FU J, LIU J, TIAN H, 等. Dual attention network for scene segmentation[C]&#x2F;&#x2F;CVPR, 2019.<br>[17] FU Y, WEI Y, ZHOU Y, 等. Horizontal pyramid matching for person re-identification[J]. arXiv preprint arXiv:1804.05275, 2018.<br>[18] GLASNER D, BAGON S, IRANI M. Super-resolution from a single image[C]&#x2F;&#x2F;ICCV, 2009: 349-356.<br>[19] GUO M, CHOU E, HUANG D A, 等. Neural graph matching networks for fewshot 3d action recognition[C]&#x2F;&#x2F;ECCV, 2018: 653-669.<br>[20] HE K, GKIOXARI G, DOLLAR P, 等. Mask r-cnn[C]&#x2F;&#x2F;ICCV, 2017: 2961-2969.<br>[21] HE K, ZHANG X, REN S, 等. Deep residual learning for image recognition[C]&#x2F;&#x2F;CVPR, 2016.<br>[22] HE L, SUN Z, ZHU Y, 等. Recognizing partial biometric patterns[J]. arXiv preprint arXiv:1810.07399, 2018.<br>[23] HERMANS A, BEYER L, LEIBE B. In defense of the triplet loss for person re-identification[J]. arXiv preprint arXiv:1703.07737, 2017.<br>[24] HU J, SHEN L, SUN G. Squeeze-and-excitation networks[C]&#x2F;&#x2F;CVPR, 2018: 7132-7141.<br>[25] HUANG G, LIU Z, VAN DER MAATEN L, 等. Densely connected convolutional networks[C]&#x2F;&#x2F;CVPR, 2017: 4700-4708.<br>[26] KALAYEH M M, BASARAN E, GOKMEN M, 等. Human semantic parsing for person re-identification[C]&#x2F;&#x2F;CVPR, 2018.<br>[27] KRIZHEVSKY A, HINTON G. Learning multiple layers of features from tiny images[R]. Technical report, Citeseer, 2009.<br>[28] LAVI B, SERJ M F, ULLAH I. Survey on deep learning techniques for person re-identification task[J]. arXiv preprint arXiv:1807.05284, 2018.<br>[29] LI W, ZHAO R, XIAO T, 等. Deepreid: Deep filter pairing neural network for person re-identification[C]&#x2F;&#x2F;CVPR, 2014.<br>[30] LI W, ZHU X, GONG S. Harmonious attention network for person re-identification[C]&#x2F;&#x2F;CVPR, 2018: 2285-2294.<br>[31] LIN T Y, MAIRE M, BELONGIE S, 等. Microsoft coco: Common objects in context[C]&#x2F;&#x2F;ECCV, 2014: 740-755.<br>[32] LIU H, FENG J, QI M, 等. End-to-end comparative attention networks for person re-identification[J]. TIP: 3492-3506.<br>[33] LIU Y, YUAN Z, ZHOU W, 等. Spatial and temporal mutual promotion for video-based person re-identification[C]&#x2F;&#x2F;AAAI, 2019.</p>
<p><strong>写在最后的话</strong></p>
<p>本文是 cvbb 暑期 cv 课程的大作业，这门课也是 cvbb 很毒瘤的经典自学课，由于网上的数据，论文等资料比较分散，原论文作者公开的源码也存在环境不适配等问题，笔者在这里进行统一的总结，如果你对 ml 或 cv 感兴趣，又或者你不幸选了这门课，只想早点水完作业去做别的事，都希望本文能让后来人少走一些弯路</p>
<p>论文原文、代码与数据集：后续会上传到对应链接</p>
<p>代码复现需要有 pytorch 环境，在本地 pycharm 中即可运行，requirements 中的依赖库最好一个一个安装，不要一起安装，可能会报玄学错误，其中 torchvision 库运行时需要保证 conda 环境中只有一个 torch（不要同时存在 CPU 版本和 GPU 版本），其中支持 torchvision 的 pillow 库需要升级到 10.0.3 版本以上，在 conda 环境下安装第三方库的话，如果 pip install 失败就试试 conda install，反之亦然，格外需要注意升级或删库的时候，旧依赖的残存文件最好也要删干净，在运行代码之前，最好先执行这段代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.version.cuda) </span><br><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())  </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> PIL</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(PIL.__version__)</span><br><span class="line"><span class="built_in">print</span>(torchvision.__version__)</span><br><span class="line"><span class="built_in">print</span>(imageio.__version__)</span><br></pre></td></tr></table></figure>

<p>能够正确输出各个版本号以后，再开始正常代码的运行，最后，感谢拉比同学提供的技术支持</p>
</article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/images/picture.jpg" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/images/picture.jpg" title="头像" alt="头像"></a><div class="post-copyright__author_name">Destiny</div><div class="post-copyright__author_desc">跋涉浮尘烛光灭</div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="http://example.com/2025/08/04/%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%E5%AD%A6%E4%B9%A0/">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('http://example.com/2025/08/04/%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%E5%AD%A6%E4%B9%A0/')">行人重识别</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"><div class="post-reward" onclick="anzhiyu.addRewardMask()"><div class="reward-button button--animated" title="赞赏作者"><i class="anzhiyufont anzhiyu-icon-hand-heart-fill"></i>打赏作者</div><div class="reward-main"><div class="reward-all"><span class="reward-title">感谢你赐予我前进的力量</span><ul class="reward-group"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul><a class="reward-main-btn" href="/about/#about-reward" target="_blank"><div class="reward-text">赞赏者名单</div><div class="reward-dec">因为你们的支持让我意识到写文章的价值🙏</div></a></div></div></div><div id="quit-box" onclick="anzhiyu.removeRewardMask()" style="display: none"></div></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="http://example.com/2025/08/04/%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%E5%AD%A6%E4%B9%A0/"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=行人重识别&amp;url=http://example.com/2025/08/04/%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%E5%AD%A6%E4%B9%A0/&amp;pic=/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A233.jpg" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><script>function copyCurrentPageUrl() {
  var currentPageUrl = window.location.href;
  var input = document.createElement("input");
  input.setAttribute("value", currentPageUrl);
  document.body.appendChild(input);
  input.select();
  input.setSelectionRange(0, 99999);
  document.execCommand("copy");
  document.body.removeChild(input);
}</script><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="copyCurrentPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">MyBlog</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"><a class="post-meta__box__tags" href="/tags/Python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>Python与机器学习<span class="tagsPageCount">13</span></a></div></div></div><div class="post_share"><div class="social-share" data-image="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A234.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/08/02/%E5%BF%AB%E8%AF%BB%E5%BF%AB%E5%86%99/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A231.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">快读快写</div></div></a></div><div class="next-post pull-right"><a href="/2025/08/04/%E5%A5%87%E5%A6%99%E6%95%85%E4%BA%8B/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A234.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">奇妙故事</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="anzhiyufont anzhiyu-icon-thumbs-up fa-fw" style="font-size: 1.5rem; margin-right: 4px"></i><span>喜欢这篇文章的人也看了</span></div><div class="relatedPosts-list"><div><a href="/2025/07/19/AI%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/" title="AI应用与开发"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A216.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-07-19</div><div class="title">AI应用与开发</div></div></a></div><div><a href="/2025/07/25/KNN%E7%AE%97%E6%B3%95/" title="ML入门-KNN算法"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A224.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-07-25</div><div class="title">ML入门-KNN算法</div></div></a></div><div><a href="/2025/07/31/ML%E4%BB%A3%E7%A0%81/" title="ML代码"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A228.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-07-31</div><div class="title">ML代码</div></div></a></div><div><a href="/2025/07/28/ML%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/" title="ML前置知识"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A226.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-07-28</div><div class="title">ML前置知识</div></div></a></div><div><a href="/2025/07/16/python-%E5%87%BD%E6%95%B0/" title="python函数"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A28.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-07-16</div><div class="title">python函数</div></div></a></div><div><a href="/2025/07/21/PyQt/" title="PyQt"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A228.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-07-21</div><div class="title">PyQt</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/images/picture.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__description"></div></div></div><div class="card-widget anzhiyu-right-widget" id="card-wechat" onclick="null"><div id="flip-wrapper"><div id="flip-content"><div class="face" style="background: url(https://bu.dusays.com/2023/01/13/63c02edf44033.png) center center / 100% no-repeat"></div><div class="back face" style="background: url(https://bu.dusays.com/2023/05/13/645fa415e8694.png) center center / 100% no-repeat"></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>文章目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%EF%BC%88%E5%9F%BA%E4%BA%8ERelation-Aware-Global-Attention%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">行人重识别（基于Relation-Aware Global Attention论文学习与代码复现）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E4%BB%BB%E5%8A%A1%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.1.</span> <span class="toc-text">一、任务介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E4%BB%80%E4%B9%88%E6%98%AF%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1 什么是行人重识别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E7%A0%94%E7%A9%B6%E8%83%8C%E6%99%AF"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.2 研究背景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.1.3.</span> <span class="toc-text">1.3 常用数据集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E8%AF%84%E4%BC%B0%E6%A0%87%E5%87%86"><span class="toc-number">1.2.</span> <span class="toc-text">二、评估标准</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-rank1%E6%A0%87%E5%87%86"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 rank1标准</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-map%E6%A0%87%E5%87%86"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 map标准</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.3.</span> <span class="toc-text">2.3 损失函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E6%A0%B8%E5%BF%83%E7%AE%97%E6%B3%95"><span class="toc-number">1.3.</span> <span class="toc-text">三、核心算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-channel-attention"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 channel attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-spatial-attention"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 spatial attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E6%A0%B8%E5%BF%83%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E6%B5%81%E7%A8%8B"><span class="toc-number">1.3.3.</span> <span class="toc-text">3.3 核心特征提取流程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0"><span class="toc-number">1.4.</span> <span class="toc-text">四、代码复现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E6%95%B0%E6%8D%AE%E9%9B%86%E9%80%89%E5%8F%96"><span class="toc-number">1.4.1.</span> <span class="toc-text">4.1 数据集选取</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E8%B6%85%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="toc-number">1.4.2.</span> <span class="toc-text">4.2 超参数设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E6%A0%B8%E5%BF%83%E4%BB%A3%E7%A0%81%E6%A1%86%E6%9E%B6"><span class="toc-number">1.4.3.</span> <span class="toc-text">4.3 核心代码框架</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E5%B1%95%E6%9C%9B"><span class="toc-number">1.5.</span> <span class="toc-text">五、模型评估与展望</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E7%BB%93%E6%9E%9C%E8%AF%84%E4%BC%B0"><span class="toc-number">1.5.1.</span> <span class="toc-text">5.1 结果评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E6%A8%A1%E5%9E%8B%E6%94%B9%E8%BF%9B%E4%B8%8E%E5%90%8E%E7%BB%AD%E5%B1%95%E6%9C%9B"><span class="toc-number">1.5.2.</span> <span class="toc-text">5.2 模型改进与后续展望</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">1.6.</span> <span class="toc-text">六、参考文献</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/08/04/%E5%A5%87%E5%A6%99%E6%95%85%E4%BA%8B/" title="奇妙故事"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A234.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="奇妙故事"/></a><div class="content"><a class="title" href="/2025/08/04/%E5%A5%87%E5%A6%99%E6%95%85%E4%BA%8B/" title="奇妙故事">奇妙故事</a><time datetime="2025-08-04T05:10:52.005Z" title="发表于 2025-08-04 13:10:52">2025-08-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/04/%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%E5%AD%A6%E4%B9%A0/" title="行人重识别"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A233.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="行人重识别"/></a><div class="content"><a class="title" href="/2025/08/04/%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%E5%AD%A6%E4%B9%A0/" title="行人重识别">行人重识别</a><time datetime="2025-08-04T00:40:54.984Z" title="发表于 2025-08-04 08:40:54">2025-08-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/02/%E5%BF%AB%E8%AF%BB%E5%BF%AB%E5%86%99/" title="快读快写"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A231.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="快读快写"/></a><div class="content"><a class="title" href="/2025/08/02/%E5%BF%AB%E8%AF%BB%E5%BF%AB%E5%86%99/" title="快读快写">快读快写</a><time datetime="2025-08-02T13:31:45.000Z" title="发表于 2025-08-02 21:31:45">2025-08-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/02/%E6%95%B0%E8%AE%BA%E4%B8%B2%E8%AE%B2/" title="数论串讲"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A230.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数论串讲"/></a><div class="content"><a class="title" href="/2025/08/02/%E6%95%B0%E8%AE%BA%E4%B8%B2%E8%AE%B2/" title="数论串讲">数论串讲</a><time datetime="2025-08-02T03:55:13.000Z" title="发表于 2025-08-02 11:55:13">2025-08-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/31/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" title="线性算法"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../images/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A229.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="线性算法"/></a><div class="content"><a class="title" href="/2025/07/31/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" title="线性算法">线性算法</a><time datetime="2025-07-31T02:29:50.000Z" title="发表于 2025-07-31 10:29:50">2025-07-31</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2020 - 2025 By <a class="footer-bar-link" href="/" title="Destiny" target="_blank">Destiny</a></div></div><div id="footer-type-tips"></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu" title="主题">主题</a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">文章</div><div class="length-num">28</div></a><a href="/tags/" title="tag"><div class="headline">标签</div><div class="length-num">2</div></a><a href="/categories/" title="category"><div class="headline">分类</div><div class="length-num">2</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" rel="external nofollow noreferrer" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.anheyu.com/" title="博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/favicon.ico" alt="博客"/><span class="back-menu-item-text">博客</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener external nofollow noreferrer" href="https://image.anheyu.com/" title="安知鱼图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="安知鱼图床"/><span class="back-menu-item-text">安知鱼图床</span></a></div></div></div><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/categories/"><span> 分类</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/tags/"><span> 标签</span></a></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/Python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">Python与机器学习<sup>13</sup></a><a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 0.88rem;">算法<sup>13</sup></a></div></div><hr/></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">播放音乐</a><div id="console-music-bg"></div><meting-js id="8152976493" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random" volume="0.7"></meting-js></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8802438608&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script>// 消除控制台打印
var HoldLog = console.log;
console.log = function () {};
let now1 = new Date();
queueMicrotask(() => {
  const Log = function () {
    HoldLog.apply(console, arguments);
  }; //在恢复前输出日志
  const grt = new Date("04/01/2021 00:00:00"); //此处修改你的建站时间或者网站上线时间
  now1.setTime(now1.getTime() + 250);
  const days = (now1 - grt) / 1000 / 60 / 60 / 24;
  const dnum = Math.floor(days);
  const ascll = [
    `欢迎使用安知鱼!`,
    `生活明朗, 万物可爱`,
    `
        
       █████╗ ███╗   ██╗███████╗██╗  ██╗██╗██╗   ██╗██╗   ██╗
      ██╔══██╗████╗  ██║╚══███╔╝██║  ██║██║╚██╗ ██╔╝██║   ██║
      ███████║██╔██╗ ██║  ███╔╝ ███████║██║ ╚████╔╝ ██║   ██║
      ██╔══██║██║╚██╗██║ ███╔╝  ██╔══██║██║  ╚██╔╝  ██║   ██║
      ██║  ██║██║ ╚████║███████╗██║  ██║██║   ██║   ╚██████╔╝
      ╚═╝  ╚═╝╚═╝  ╚═══╝╚══════╝╚═╝  ╚═╝╚═╝   ╚═╝    ╚═════╝
        
        `,
    "已上线",
    dnum,
    "天",
    "©2020 By 安知鱼 V1.6.14",
  ];
  const ascll2 = [`NCC2-036`, `调用前置摄像头拍照成功，识别为【小笨蛋】.`, `Photo captured: `, `🤪`];

  setTimeout(
    Log.bind(
      console,
      `\n%c${ascll[0]} %c ${ascll[1]} %c ${ascll[2]} %c${ascll[3]}%c ${ascll[4]}%c ${ascll[5]}\n\n%c ${ascll[6]}\n`,
      "color:#425AEF",
      "",
      "color:#425AEF",
      "color:#425AEF",
      "",
      "color:#425AEF",
      ""
    )
  );
  setTimeout(
    Log.bind(
      console,
      `%c ${ascll2[0]} %c ${ascll2[1]} %c \n${ascll2[2]} %c\n${ascll2[3]}\n`,
      "color:white; background-color:#4fd953",
      "",
      "",
      'background:url("https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/tinggge.gif") no-repeat;font-size:450%'
    )
  );

  setTimeout(Log.bind(console, "%c WELCOME %c 你好，小笨蛋.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(
      console,
      "%c ⚡ Powered by 安知鱼 %c 你正在访问 Destiny 的博客.",
      "color:white; background-color:#f0ad4e",
      ""
    )
  );

  setTimeout(Log.bind(console, "%c W23-12 %c 你已打开控制台.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(console, "%c S013-782 %c 你现在正处于监控中.", "color:white; background-color:#d9534f", "")
  );
});</script><script async src="/anzhiyu/random.js"></script><div class="js-pjax"><input type="hidden" name="page-type" id="page-type" value="post"></div><script async data-pjax src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.1/bubble/bubble.js"></script><script>var visitorMail = "";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/dist/fireworks.min.js"></script><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>